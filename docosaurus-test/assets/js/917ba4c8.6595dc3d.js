"use strict";(self.webpackChunkdocosaurus_test=self.webpackChunkdocosaurus_test||[]).push([[9504],{2763:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"first-post","metadata":{"permalink":"/docosaurus-test/blog/first-post","editUrl":"https://github.com/ruzickap/blog-test.ruzicka.dev/tree/main/docosaurus-test/blog/2021/2021-10-03-first_post.md","source":"@site/blog/2021/2021-10-03-first_post.md","title":"First_post","description":"Test123 description","date":"2021-10-03T00:00:00.000Z","tags":[{"inline":true,"label":"first","permalink":"/docosaurus-test/blog/tags/first"},{"inline":true,"label":"first_post","permalink":"/docosaurus-test/blog/tags/first-post"}],"readingTime":0.035,"hasTruncateMarker":false,"authors":[{"name":"Petr Ruzicka","title":"Blog owner","url":"https://petr.ruzicka.dev","imageURL":"https://github.com/ruzickap.png","key":"Petr Ruzicka"}],"frontMatter":{"slug":"first-post","title":"First_post","authors":"Petr Ruzicka","date":"2021-10-03T00:00:00.000Z","description":"Test123 description","categories":["Test1","Test2"],"tags":["first","first_post"]},"unlisted":false,"nextItem":{"title":"Welcome","permalink":"/docosaurus-test/blog/welcome"}},"content":"Test 123\\n\\n```yaml\\nResource:\\n- \\"arn:aws:secretsmanager:*:*:secret:*\\"\\n```"},{"id":"welcome","metadata":{"permalink":"/docosaurus-test/blog/welcome","editUrl":"https://github.com/ruzickap/blog-test.ruzicka.dev/tree/main/docosaurus-test/blog/2021-08-26-welcome/index.md","source":"@site/blog/2021-08-26-welcome/index.md","title":"Welcome","description":"Docusaurus blogging features are powered by the blog plugin.","date":"2021-08-26T00:00:00.000Z","tags":[{"inline":true,"label":"facebook","permalink":"/docosaurus-test/blog/tags/facebook"},{"inline":true,"label":"hello","permalink":"/docosaurus-test/blog/tags/hello"},{"inline":true,"label":"docusaurus","permalink":"/docosaurus-test/blog/tags/docusaurus"}],"readingTime":0.405,"hasTruncateMarker":false,"authors":[{"name":"S\xe9bastien Lorber","title":"Docusaurus maintainer","url":"https://sebastienlorber.com","imageURL":"https://github.com/slorber.png","key":"slorber"},{"name":"Yangshun Tay","title":"Front End Engineer @ Facebook","url":"https://github.com/yangshun","imageURL":"https://github.com/yangshun.png","key":"yangshun"}],"frontMatter":{"slug":"welcome","title":"Welcome","authors":["slorber","yangshun"],"tags":["facebook","hello","docusaurus"]},"unlisted":false,"prevItem":{"title":"First_post","permalink":"/docosaurus-test/blog/first-post"},"nextItem":{"title":"MDX Blog Post","permalink":"/docosaurus-test/blog/mdx-blog-post"}},"content":"[Docusaurus blogging features](https://docusaurus.io/docs/blog) are powered by the [blog plugin](https://docusaurus.io/docs/api/plugins/@docusaurus/plugin-content-blog).\\n\\nSimply add Markdown files (or folders) to the `blog` directory.\\n\\nRegular blog authors can be added to `authors.yml`.\\n\\nThe blog post date can be extracted from filenames, such as:\\n\\n- `2019-05-30-welcome.md`\\n- `2019-05-30-welcome/index.md`\\n\\nA blog post folder can be convenient to co-locate blog post images:\\n\\n![Docusaurus Plushie](./docusaurus-plushie-banner.jpeg)\\n\\nThe blog supports tags as well!\\n\\n**And if you don\'t want a blog**: just delete this directory, and use `blog: false` in your Docusaurus config."},{"id":"mdx-blog-post","metadata":{"permalink":"/docosaurus-test/blog/mdx-blog-post","editUrl":"https://github.com/ruzickap/blog-test.ruzicka.dev/tree/main/docosaurus-test/blog/2021-08-01-mdx-blog-post.mdx","source":"@site/blog/2021-08-01-mdx-blog-post.mdx","title":"MDX Blog Post","description":"Blog posts support Docusaurus Markdown features, such as MDX.","date":"2021-08-01T00:00:00.000Z","tags":[{"inline":true,"label":"docusaurus","permalink":"/docosaurus-test/blog/tags/docusaurus"}],"readingTime":0.175,"hasTruncateMarker":false,"authors":[{"name":"S\xe9bastien Lorber","title":"Docusaurus maintainer","url":"https://sebastienlorber.com","imageURL":"https://github.com/slorber.png","key":"slorber"}],"frontMatter":{"slug":"mdx-blog-post","title":"MDX Blog Post","authors":["slorber"],"tags":["docusaurus"]},"unlisted":false,"prevItem":{"title":"Welcome","permalink":"/docosaurus-test/blog/welcome"},"nextItem":{"title":"Test EKS Post","permalink":"/docosaurus-test/blog/test-eks-post"}},"content":"Blog posts support [Docusaurus Markdown features](https://docusaurus.io/docs/markdown-features), such as [MDX](https://mdxjs.com/).\\n\\n:::tip\\n\\nUse the power of React to create interactive blog posts.\\n\\n```js\\n<button onClick={() => alert(\'button clicked!\')}>Click me!</button>\\n```\\n\\n<button onClick={() => alert(\'button clicked!\')}>Click me!</button>\\n\\n:::"},{"id":"test-eks-post","metadata":{"permalink":"/docosaurus-test/blog/test-eks-post","editUrl":"https://github.com/ruzickap/blog-test.ruzicka.dev/tree/main/docosaurus-test/blog/2020/2020-12-10-eks-test.md","source":"@site/blog/2020/2020-12-10-eks-test.md","title":"Test EKS Post","description":"![Amazon EKS](https://raw.githubusercontent.com/cncf/landscape/7f5b02ecba914a32912e77fc78e1c54d1c2f98ec/hosted_logos/amazon-eks.svg?sanitize=true","date":"2020-12-10T00:00:00.000Z","tags":[{"inline":true,"label":"EKS","permalink":"/docosaurus-test/blog/tags/eks"},{"inline":true,"label":"Pipeline Templates","permalink":"/docosaurus-test/blog/tags/pipeline-templates"}],"readingTime":21.465,"hasTruncateMarker":false,"authors":[{"name":"Petr Ruzicka","title":"Blog owner","url":"https://petr.ruzicka.dev","imageURL":"https://github.com/ruzickap.png","key":"Petr Ruzicka"}],"frontMatter":{"slug":"test-eks-post","title":"Test EKS Post","authors":"Petr Ruzicka","date":"2020-12-10T00:00:00.000Z","categories":["EKS","Pipelines"],"tags":["EKS","Pipeline Templates"]},"unlisted":false,"prevItem":{"title":"MDX Blog Post","permalink":"/docosaurus-test/blog/mdx-blog-post"},"nextItem":{"title":"Image Test EKS Post","permalink":"/docosaurus-test/blog/image-test-eks-post"}},"content":"![Amazon EKS](https://raw.githubusercontent.com/cncf/landscape/7f5b02ecba914a32912e77fc78e1c54d1c2f98ec/hosted_logos/amazon-eks.svg?sanitize=true\\n\\"Amazon EKS\\")\\n\\nBefore starting with the main content, it\'s necessary to provision\\nthe [Amazon EKS](https://aws.amazon.com/eks/) in AWS.\\n\\n## Requirements\\n\\nIf you would like to follow this documents and it\'s task you will need to set up\\nfew environment variables.\\n\\nThe `LETSENCRYPT_ENVIRONMENT` variable should be one of:\\n\\n* `staging` - Let\u2019s Encrypt will create testing certificate (not valid)\\n* `production` - Let\u2019s Encrypt will create valid certificate (use with care)\\n\\n`BASE_DOMAIN` contains DNS records for all your Kubernetes clusters. The cluster\\nnames will look like `CLUSTER_NAME`.`BASE_DOMAIN` (`kube1.k8s.mylabs.dev`).\\n\\n```bash\\n# Hostname / FQDN definitions\\nexport BASE_DOMAIN=${BASE_DOMAIN:-k8s.mylabs.dev}\\nexport CLUSTER_NAME=${CLUSTER_NAME:-kube1}\\nexport CLUSTER_FQDN=\\"${CLUSTER_NAME}.${BASE_DOMAIN}\\"\\nexport KUBECONFIG=${PWD}/kubeconfig-${CLUSTER_NAME}.conf\\n# * \\"production\\" - valid certificates signed by Lets Encrypt \\"\\"\\n# * \\"staging\\" - not trusted certs signed by Lets Encrypt \\"Fake LE Intermediate X1\\"\\nexport LETSENCRYPT_ENVIRONMENT=\\"staging\\"\\nexport LETSENCRYPT_CERTIFICATE=\\"https://letsencrypt.org/certs/staging/letsencrypt-stg-root-x1.pem\\"\\n# export LETSENCRYPT_ENVIRONMENT=\\"production\\"\\n# export LETSENCRYPT_CERTIFICATE=\\"https://letsencrypt.org/certs/lets-encrypt-r3.pem\\"\\nexport MY_EMAIL=\\"petr.ruzicka@gmail.com\\"\\n# GitHub Organization + Team where are the users who will have the admin access\\n# to K8s resources (Grafana). Only users in GitHub organization\\n# (MY_GITHUB_ORG_NAME) will be able to access the apps via ingress.\\nexport MY_GITHUB_ORG_NAME=\\"ruzickap-org\\"\\nexport MY_GITHUB_USERNAME=\\"ruzickap\\"\\n# AWS Region\\nexport AWS_DEFAULT_REGION=\\"eu-west-1\\"\\nexport SLACK_CHANNEL=\\"mylabs\\"\\n# Tags used to tag the AWS resources\\nexport TAGS=\\"Owner=${MY_EMAIL} Environment=Dev Group=Cloud_Native Squad=Cloud_Container_Platform compliance:na:defender=bottlerocket\\"\\necho -e \\"${MY_EMAIL} | ${LETSENCRYPT_ENVIRONMENT} | ${CLUSTER_NAME} | ${BASE_DOMAIN} | ${CLUSTER_FQDN}\\\\n${TAGS}\\"\\n```\\n\\nPrepare GitHub OAuth \\"access\\" credentials ans AWS \\"access\\" variables.\\n\\nYou will need to configure AWS CLI: [Configuring the AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html)\\n\\n```shell\\n# Common password\\nexport MY_PASSWORD=\\"xxxx\\"\\n# AWS Credentials\\nexport AWS_ACCESS_KEY_ID=\\"\\"\\nexport AWS_SECRET_ACCESS_KEY=\\"\\"\\nexport AWS_CONSOLE_ADMIN_ROLE_ARN=\\"arn:aws:iam::7xxxxxxxxxx7:role/xxxxxxxxxxxxxN\\"\\n# GitHub Organization OAuth Apps credentials\\nexport MY_GITHUB_ORG_OAUTH_DEX_CLIENT_ID=\\"3xxxxxxxxxxxxxxxxxx3\\"\\nexport MY_GITHUB_ORG_OAUTH_DEX_CLIENT_SECRET=\\"7xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx8\\"\\nexport MY_GITHUB_ORG_OAUTH_KEYCLOAK_CLIENT_ID=\\"4xxxxxxxxxxxxxxxxxx4\\"\\nexport MY_GITHUB_ORG_OAUTH_KEYCLOAK_CLIENT_SECRET=\\"7xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxa\\"\\n# Sysdig credentials\\nexport SYSDIG_AGENT_ACCESSKEY=\\"xxx\\"\\n# Aqua credentials\\nexport AQUA_REGISTRY_USERNAME=\\"xxx\\"\\nexport AQUA_REGISTRY_PASSWORD=\\"xxx\\"\\nexport AQUA_ENFORCER_TOKEN=\\"xxx\\"\\n# Splunk credentials\\nexport SPLUNK_HOST=\\"xxx\\"\\nexport SPLUNK_TOKEN=\\"xxx\\"\\nexport SPLUNK_INDEX_NAME=\\"xxx\\"\\n# Slack incoming webhook\\nexport SLACK_INCOMING_WEBHOOK_URL=\\"https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK\\"\\nexport SLACK_BOT_API_TOKEN=\\"xxxx-xxxxxxxxxxxxx-xxxxxxxxxxxxx-xxxxxxxxxxxxxxxxxxxxxxxP\\"\\n# Okta configuration\\nexport OKTA_ISSUER=\\"https://exxxxxxx-xxxxx-xx.okta.com\\"\\nexport OKTA_CLIENT_ID=\\"0xxxxxxxxxxxxxxxxxx7\\"\\nexport OKTA_CLIENT_SECRET=\\"1xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxH\\"\\n```\\n\\nVerify if all the necessary variables were set:\\n\\n```bash\\ncase \\"${CLUSTER_NAME}\\" in\\n  kube1)\\n    MY_GITHUB_ORG_OAUTH_DEX_CLIENT_ID=${MY_GITHUB_ORG_OAUTH_DEX_CLIENT_ID:-${MY_GITHUB_ORG_OAUTH_DEX_CLIENT_ID_KUBE1}}\\n    MY_GITHUB_ORG_OAUTH_DEX_CLIENT_SECRET=${MY_GITHUB_ORG_OAUTH_DEX_CLIENT_SECRET:-${MY_GITHUB_ORG_OAUTH_DEX_CLIENT_SECRET_KUBE1}}\\n    MY_GITHUB_ORG_OAUTH_KEYCLOAK_CLIENT_ID=${MY_GITHUB_ORG_OAUTH_KEYCLOAK_CLIENT_ID:-${MY_GITHUB_ORG_OAUTH_KEYCLOAK_CLIENT_ID_KUBE1}}\\n    MY_GITHUB_ORG_OAUTH_KEYCLOAK_CLIENT_SECRET=${MY_GITHUB_ORG_OAUTH_KEYCLOAK_CLIENT_SECRET:-${MY_GITHUB_ORG_OAUTH_KEYCLOAK_CLIENT_SECRET_KUBE1}}\\n    ;;\\n  kube2)\\n    MY_GITHUB_ORG_OAUTH_DEX_CLIENT_ID=${MY_GITHUB_ORG_OAUTH_DEX_CLIENT_ID:-${MY_GITHUB_ORG_OAUTH_DEX_CLIENT_ID_KUBE2}}\\n    MY_GITHUB_ORG_OAUTH_DEX_CLIENT_SECRET=${MY_GITHUB_ORG_OAUTH_DEX_CLIENT_SECRET:-${MY_GITHUB_ORG_OAUTH_DEX_CLIENT_SECRET_KUBE2}}\\n    MY_GITHUB_ORG_OAUTH_KEYCLOAK_CLIENT_ID=${MY_GITHUB_ORG_OAUTH_KEYCLOAK_CLIENT_ID:-${MY_GITHUB_ORG_OAUTH_KEYCLOAK_CLIENT_ID_KUBE2}}\\n    MY_GITHUB_ORG_OAUTH_KEYCLOAK_CLIENT_SECRET=${MY_GITHUB_ORG_OAUTH_KEYCLOAK_CLIENT_SECRET:-${MY_GITHUB_ORG_OAUTH_KEYCLOAK_CLIENT_SECRET_KUBE2}}\\n    ;;\\n  *)\\n    echo \\"Unsupported cluster name: ${CLUSTER_NAME} !\\"\\n    exit 1\\n    ;;\\nesac\\n\\n: \\"${AWS_ACCESS_KEY_ID?}\\"\\n: \\"${AWS_SECRET_ACCESS_KEY?}\\"\\n: \\"${AWS_CONSOLE_ADMIN_ROLE_ARN?}\\"\\n: \\"${GITHUB_TOKEN?}\\"\\n: \\"${SLACK_INCOMING_WEBHOOK_URL?}\\"\\n: \\"${SLACK_BOT_API_TOKEN?}\\"\\n: \\"${MY_PASSWORD?}\\"\\n: \\"${OKTA_ISSUER?}\\"\\n: \\"${OKTA_CLIENT_ID?}\\"\\n: \\"${OKTA_CLIENT_SECRET?}\\"\\n```\\n\\n## Prepare the local working environment\\n\\n::: tip\\nYou can skip these steps if you have all the required software already\\ninstalled.\\n:::\\n\\nInstall necessary software:\\n\\n```bash\\n\\nif command -v apt-get &> /dev/null; then\\n  apt update -qq\\n  DEBIAN_FRONTEND=noninteractive apt-get install -y -qq apache2-utils ansible dnsutils git gnupg2 jq sudo unzip > /dev/null\\nfi\\n```\\n\\nInstall [AWS CLI](https://aws.amazon.com/cli/)  binary:\\n\\n```bash\\nif ! command -v aws &> /dev/null; then\\n  curl -sL \\"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\\" -o \\"/tmp/awscliv2.zip\\"\\n  unzip -q -o /tmp/awscliv2.zip -d /tmp/\\n  sudo /tmp/aws/install\\nfi\\n```\\n\\nInstall [kubectl](https://github.com/kubernetes/kubectl) binary:\\n\\n```bash\\nif ! command -v kubectl &> /dev/null; then\\n  # https://github.com/kubernetes/kubectl/releases\\n  sudo curl -s -Lo /usr/local/bin/kubectl \\"https://storage.googleapis.com/kubernetes-release/release/v1.21.1/bin/$(uname | sed \\"s/./\\\\L&/g\\")/amd64/kubectl\\"\\n  sudo chmod a+x /usr/local/bin/kubectl\\nfi\\n```\\n\\nInstall [Helm](https://helm.sh/):\\n\\n```bash\\nif ! command -v helm &> /dev/null; then\\n  # https://github.com/helm/helm/releases\\n  curl -s https://raw.githubusercontent.com/helm/helm/master/scripts/get | bash -s -- --version v3.6.0\\nfi\\n```\\n\\nInstall [eksctl](https://eksctl.io/):\\n\\n```bash\\nif ! command -v eksctl &> /dev/null; then\\n  # https://github.com/weaveworks/eksctl/releases\\n  curl -s -L \\"https://github.com/weaveworks/eksctl/releases/download/0.60.0/eksctl_$(uname)_amd64.tar.gz\\" | sudo tar xz -C /usr/local/bin/\\nfi\\n```\\n\\nInstall [AWS IAM Authenticator for Kubernetes](https://github.com/kubernetes-sigs/aws-iam-authenticator):\\n\\n```bash\\nif ! command -v aws-iam-authenticator &> /dev/null; then\\n  # https://docs.aws.amazon.com/eks/latest/userguide/install-aws-iam-authenticator.html\\n  sudo curl -s -Lo /usr/local/bin/aws-iam-authenticator \\"https://amazon-eks.s3.us-west-2.amazonaws.com/1.19.6/2021-01-05/bin/$(uname | sed \\"s/./\\\\L&/g\\")/amd64/aws-iam-authenticator\\"\\n  sudo chmod a+x /usr/local/bin/aws-iam-authenticator\\nfi\\n```\\n\\nInstall [vault](https://www.vaultproject.io/downloads):\\n\\n```bash\\nif ! command -v vault &> /dev/null; then\\n  curl -s -L \\"https://releases.hashicorp.com/vault/1.7.2/vault_1.7.2_$(uname | sed \\"s/./\\\\L&/g\\")_amd64.zip\\" -o /tmp/vault.zip\\n  sudo unzip -q /tmp/vault.zip -d /usr/local/bin/\\n  rm /tmp/vault.zip\\nfi\\n```\\n\\nInstall [velero](https://github.com/vmware-tanzu/velero/releases):\\n\\n```bash\\nif ! command -v velero &> /dev/null; then\\n  curl -s -L \\"https://github.com/vmware-tanzu/velero/releases/download/v1.6.0/velero-v1.6.0-$(uname | sed \\"s/./\\\\L&/g\\")-amd64.tar.gz\\" -o /tmp/velero.tar.gz\\n  sudo tar xzf /tmp/velero.tar.gz -C /usr/local/bin/ --strip-components 1 \\"velero-v1.6.0-$(uname | sed \\"s/./\\\\L&/g\\")-amd64/velero\\"\\nfi\\n```\\n\\nInstall [flux](https://toolkit.fluxcd.io/):\\n\\n```bash\\nif ! command -v flux &> /dev/null; then\\n  curl -s https://fluxcd.io/install.sh | sudo bash\\nfi\\n```\\n\\nInstall `calicoctl`:\\n\\n```bash\\nif ! command -v calicoctl &> /dev/null; then\\n  sudo curl -s -Lo /usr/local/bin/calicoctl https://github.com/projectcalico/calicoctl/releases/download/v3.20.0/calicoctl\\n  sudo chmod a+x /usr/local/bin/calicoctl\\nfi\\n```\\n\\nInstall [SOPS: Secrets OPerationS](https://github.com/mozilla/sops):\\n\\n```bash\\nif ! command -v sops &> /dev/null; then\\n  sudo curl -s -Lo /usr/local/bin/sops \\"https://github.com/mozilla/sops/releases/download/v3.7.1/sops-v3.7.1.$(uname | sed \\"s/./\\\\L&/g\\")\\"\\n  sudo chmod a+x /usr/local/bin/sops\\nfi\\n```\\n\\nInstall [kustomize](https://kustomize.io/):\\n\\n```bash\\nif ! command -v kustomize &> /dev/null; then\\n  curl -s \\"https://raw.githubusercontent.com/kubernetes-sigs/kustomize/master/hack/install_kustomize.sh\\" | sudo bash -s 4.1.2 /usr/local/bin/\\nfi\\n```\\n\\nInstall [hey](https://github.com/rakyll/hey):\\n\\n```bash\\nif ! command -v hey &> /dev/null; then\\n  sudo curl -s -Lo /usr/local/bin/hey \\"https://hey-release.s3.us-east-2.amazonaws.com/hey_$(uname | sed \\"s/./\\\\L&/g\\")_amd64\\"\\n  sudo chmod a+x /usr/local/bin/hey\\nfi\\n```\\n\\n## Configure AWS Route 53 Domain delegation\\n\\nCreate DNS zone (`BASE_DOMAIN`):\\n\\n```shell\\naws route53 create-hosted-zone --output json \\\\\\n  --name \\"${BASE_DOMAIN}\\" \\\\\\n  --caller-reference \\"$(date)\\" \\\\\\n  --hosted-zone-config=\\"{\\\\\\"Comment\\\\\\": \\\\\\"Created by ${MY_EMAIL}\\\\\\", \\\\\\"PrivateZone\\\\\\": false}\\" | jq\\n```\\n\\nUse your domain registrar to change the nameservers for your zone (for example\\n\\"mylabs.dev\\") to use the Amazon Route 53 nameservers. Here is the way how you\\ncan find out the the Route 53 nameservers:\\n\\n```shell\\nNEW_ZONE_ID=$(aws route53 list-hosted-zones --query \\"HostedZones[?Name==\\\\`${BASE_DOMAIN}.\\\\`].Id\\" --output text)\\nNEW_ZONE_NS=$(aws route53 get-hosted-zone --output json --id \\"${NEW_ZONE_ID}\\" --query \\"DelegationSet.NameServers\\")\\nNEW_ZONE_NS1=$(echo \\"${NEW_ZONE_NS}\\" | jq -r \\".[0]\\")\\nNEW_ZONE_NS2=$(echo \\"${NEW_ZONE_NS}\\" | jq -r \\".[1]\\")\\n```\\n\\nCreate the NS record in `k8s.mylabs.dev` (`BASE_DOMAIN`) for proper zone\\ndelegation. This step depends on your domain registrar - I\'m using CloudFlare\\nand using Ansible to automate it:\\n\\n```shell\\nansible -m cloudflare_dns -c local -i \\"localhost,\\" localhost -a \\"zone=mylabs.dev record=${BASE_DOMAIN} type=NS value=${NEW_ZONE_NS1} solo=true proxied=no account_email=${CLOUDFLARE_EMAIL} account_api_token=${CLOUDFLARE_API_KEY}\\"\\nansible -m cloudflare_dns -c local -i \\"localhost,\\" localhost -a \\"zone=mylabs.dev record=${BASE_DOMAIN} type=NS value=${NEW_ZONE_NS2} solo=false proxied=no account_email=${CLOUDFLARE_EMAIL} account_api_token=${CLOUDFLARE_API_KEY}\\"\\n```\\n\\nOutput:\\n\\n```text\\nlocalhost | CHANGED => {\\n    \\"ansible_facts\\": {\\n        \\"discovered_interpreter_python\\": \\"/usr/bin/python\\"\\n    },\\n    \\"changed\\": true,\\n    \\"result\\": {\\n        \\"record\\": {\\n            \\"content\\": \\"ns-885.awsdns-46.net\\",\\n            \\"created_on\\": \\"2020-11-13T06:25:32.18642Z\\",\\n            \\"id\\": \\"dxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxb\\",\\n            \\"locked\\": false,\\n            \\"meta\\": {\\n                \\"auto_added\\": false,\\n                \\"managed_by_apps\\": false,\\n                \\"managed_by_argo_tunnel\\": false,\\n                \\"source\\": \\"primary\\"\\n            },\\n            \\"modified_on\\": \\"2020-11-13T06:25:32.18642Z\\",\\n            \\"name\\": \\"k8s.mylabs.dev\\",\\n            \\"proxiable\\": false,\\n            \\"proxied\\": false,\\n            \\"ttl\\": 1,\\n            \\"type\\": \\"NS\\",\\n            \\"zone_id\\": \\"2xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxe\\",\\n            \\"zone_name\\": \\"mylabs.dev\\"\\n        }\\n    }\\n}\\nlocalhost | CHANGED => {\\n    \\"ansible_facts\\": {\\n        \\"discovered_interpreter_python\\": \\"/usr/bin/python\\"\\n    },\\n    \\"changed\\": true,\\n    \\"result\\": {\\n        \\"record\\": {\\n            \\"content\\": \\"ns-1692.awsdns-19.co.uk\\",\\n            \\"created_on\\": \\"2020-11-13T06:25:37.605605Z\\",\\n            \\"id\\": \\"9xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxb\\",\\n            \\"locked\\": false,\\n            \\"meta\\": {\\n                \\"auto_added\\": false,\\n                \\"managed_by_apps\\": false,\\n                \\"managed_by_argo_tunnel\\": false,\\n                \\"source\\": \\"primary\\"\\n            },\\n            \\"modified_on\\": \\"2020-11-13T06:25:37.605605Z\\",\\n            \\"name\\": \\"k8s.mylabs.dev\\",\\n            \\"proxiable\\": false,\\n            \\"proxied\\": false,\\n            \\"ttl\\": 1,\\n            \\"type\\": \\"NS\\",\\n            \\"zone_id\\": \\"2xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxe\\",\\n            \\"zone_name\\": \\"mylabs.dev\\"\\n        }\\n    }\\n}\\n```\\n\\n## Add new domain to Route 53, Policies, S3, EBS\\n\\nDetails with examples are described on these links:\\n\\n* [https://aws.amazon.com/blogs/opensource/introducing-fine-grained-iam-roles-service-accounts/](https://aws.amazon.com/blogs/opensource/introducing-fine-grained-iam-roles-service-accounts/)\\n* [https://cert-manager.io/docs/configuration/acme/dns01/route53/](https://cert-manager.io/docs/configuration/acme/dns01/route53/)\\n* [https://github.com/kubernetes-sigs/external-dns/blob/master/docs/tutorials/aws.md](https://github.com/kubernetes-sigs/external-dns/blob/master/docs/tutorials/aws.md)\\n\\nCreate CloudFormation template containing policies for Route53, S3 access\\n(Harbor, Velero) and Domain. Put new domain `CLUSTER_FQDN` to the Route 53 and\\nconfigure the DNS delegation from the `BASE_DOMAIN`.\\n\\n```bash\\nmkdir -vp \\"tmp/${CLUSTER_FQDN}\\"\\n\\ncat > \\"tmp/${CLUSTER_FQDN}/aws-route53-iam-s3-kms-asm.yml\\" << \\\\EOF\\nDescription: \\"Template to generate the necessary IAM Policies for access to Route53 and S3\\"\\nParameters:\\n  ClusterFQDN:\\n    Description: \\"Cluster domain where all necessary app subdomains will live (subdomain of BaseDomain). Ex: kube1.k8s.mylabs.dev\\"\\n    Type: String\\n  ClusterName:\\n    Description: \\"Cluster Name Ex: kube1\\"\\n    Type: String\\n  BaseDomain:\\n    Description: \\"Base domain where cluster domains + their subdomains will live. Ex: k8s.mylabs.dev\\"\\n    Type: String\\nResources:\\n  # This AWS control checks whether the status of the AWS Systems Manager association compliance is COMPLIANT or NON_COMPLIANT after the association is executed on an instance.\\n  ConfigRule:\\n    Type: \\"AWS::Config::ConfigRule\\"\\n    Properties:\\n      ConfigRuleName: !Sub \\"${ClusterName}-ec2-managedinstance-association-compliance-status-check\\"\\n      Scope:\\n        ComplianceResourceTypes:\\n          - \\"AWS::SSM::AssociationCompliance\\"\\n      Description: \\"A Config rule that checks whether the compliance status of the Amazon EC2 Systems Manager association compliance is COMPLIANT or NON_COMPLIANT after the association execution on the instance. The rule is compliant if the field status is COMPLIANT.\\"\\n      Source:\\n        Owner: \\"AWS\\"\\n        SourceIdentifier: \\"EC2_MANAGEDINSTANCE_ASSOCIATION_COMPLIANCE_STATUS_CHECK\\"\\n  CloudWatchPolicy:\\n    Type: AWS::IAM::ManagedPolicy\\n    Properties:\\n      ManagedPolicyName: !Sub \\"${ClusterFQDN}-CloudWatch\\"\\n      Description: !Sub \\"Policy required by Fargate to log to CloudWatch for ${ClusterFQDN}\\"\\n      PolicyDocument:\\n        Version: \\"2012-10-17\\"\\n        Statement:\\n        - Effect: Allow\\n          Action:\\n          - logs:CreateLogStream\\n          - logs:CreateLogGroup\\n          - logs:DescribeLogStreams\\n          - logs:PutLogEvents\\n          Resource: \\"*\\"\\n  HostedZone:\\n    Type: AWS::Route53::HostedZone\\n    Properties:\\n      Name: !Ref ClusterFQDN\\n  KMSAlias:\\n    Type: AWS::KMS::Alias\\n    Properties:\\n      AliasName: !Sub \\"alias/eks-${ClusterName}\\"\\n      TargetKeyId: !Ref KMSKey\\n  KMSKey:\\n    Type: AWS::KMS::Key\\n    Properties:\\n      Description: !Sub \\"KMS key for secrets related to ${ClusterFQDN}\\"\\n      EnableKeyRotation: true\\n      PendingWindowInDays: 7\\n      KeyPolicy:\\n        Version: \\"2012-10-17\\"\\n        Id: !Sub \\"eks-key-policy-${ClusterName}\\"\\n        Statement:\\n        - Sid: Enable IAM User Permissions\\n          Effect: Allow\\n          Principal:\\n            AWS: !Sub \\"arn:aws:iam::${AWS::AccountId}:root\\"\\n          Action: kms:*\\n          Resource: \\"*\\"\\n        - Sid: Allow use of the key\\n          Effect: Allow\\n          Principal:\\n            AWS: !Sub \\"arn:aws:iam::${AWS::AccountId}:role/aws-service-role/autoscaling.amazonaws.com/AWSServiceRoleForAutoScaling\\"\\n          Action:\\n          - kms:Encrypt\\n          - kms:Decrypt\\n          - kms:ReEncrypt*\\n          - kms:GenerateDataKey*\\n          - kms:DescribeKey\\n          Resource: \\"*\\"\\n        - Sid: Allow attachment of persistent resources\\n          Effect: Allow\\n          Principal:\\n            AWS: !Sub \\"arn:aws:iam::${AWS::AccountId}:role/aws-service-role/autoscaling.amazonaws.com/AWSServiceRoleForAutoScaling\\"\\n          Action:\\n          - kms:CreateGrant\\n          Resource: \\"*\\"\\n          Condition:\\n            Bool:\\n              kms:GrantIsForAWSResource: true\\n  EKSViewNodesAndWorkloadsPolicy:\\n    Type: AWS::IAM::ManagedPolicy\\n    Properties:\\n      ManagedPolicyName: !Sub \\"${ClusterFQDN}-EKSViewNodesAndWorkloads\\"\\n      Description: !Sub \\"Policy used to view workloads running in an EKS cluster created using CAPA\\"\\n      PolicyDocument:\\n        Version: \\"2012-10-17\\"\\n        Statement:\\n        - Effect: Allow\\n          Action:\\n          - eks:DescribeNodegroup\\n          - eks:ListNodegroups\\n          - eks:DescribeCluster\\n          - eks:ListClusters\\n          - eks:AccessKubernetesApi\\n          - ssm:GetParameter\\n          - eks:ListUpdates\\n          - eks:ListFargateProfiles\\n          Resource: \\"*\\"\\n  RecordSet:\\n    Type: AWS::Route53::RecordSet\\n    Properties:\\n      HostedZoneName: !Sub \\"${BaseDomain}.\\"\\n      Name: !Ref ClusterFQDN\\n      Type: NS\\n      TTL: 60\\n      ResourceRecords: !GetAtt HostedZone.NameServers\\n  S3Policy:\\n    Type: AWS::IAM::ManagedPolicy\\n    Properties:\\n      ManagedPolicyName: !Sub \\"${ClusterFQDN}-AmazonS3\\"\\n      Description: !Sub \\"Policy required by Harbor and Velero to write to S3 bucket ${ClusterFQDN}\\"\\n      PolicyDocument:\\n        Version: \\"2012-10-17\\"\\n        Statement:\\n        - Effect: Allow\\n          Action:\\n          - s3:ListBucket\\n          - s3:GetBucketLocation\\n          - s3:ListBucketMultipartUploads\\n          Resource: !GetAtt S3Bucket.Arn\\n        - Effect: Allow\\n          Action:\\n          - s3:PutObject\\n          - s3:GetObject\\n          - s3:DeleteObject\\n          - s3:ListMultipartUploadParts\\n          - s3:AbortMultipartUpload\\n          Resource: !Sub \\"arn:aws:s3:::${ClusterFQDN}/*\\"\\n  S3Bucket:\\n    Type: AWS::S3::Bucket\\n    Properties:\\n      AccessControl: Private\\n      BucketName: !Sub \\"${ClusterFQDN}\\"\\n      BucketEncryption:\\n        ServerSideEncryptionConfiguration:\\n          - ServerSideEncryptionByDefault:\\n              SSEAlgorithm: AES256\\n  SecretsManagerMySecret:\\n    Type: AWS::SecretsManager::Secret\\n    Properties:\\n      Name: !Sub \\"${ClusterFQDN}-MySecret\\"\\n      Description: My Secret\\n      GenerateSecretString:\\n        SecretStringTemplate: \\"{\\\\\\"username\\\\\\": \\\\\\"Administrator\\\\\\"}\\"\\n        GenerateStringKey: password\\n        PasswordLength: 32\\n      KmsKeyId: !Ref KMSKey\\n  SecretsManagerMySecret2:\\n    Type: AWS::SecretsManager::Secret\\n    Properties:\\n      Name: !Sub \\"${ClusterFQDN}-MySecret2\\"\\n      Description: My Secret2\\n      GenerateSecretString:\\n        SecretStringTemplate: \\"{\\\\\\"username\\\\\\": \\\\\\"Administrator2\\\\\\"}\\"\\n        GenerateStringKey: password\\n        PasswordLength: 32\\n      KmsKeyId: !Ref KMSKey\\n  UserMyUser1:\\n    Type: AWS::IAM::User\\n    Properties:\\n      UserName: !Sub \\"myuser1-${ClusterName}\\"\\n      Policies:\\n      - PolicyName: !Sub \\"myuser1-${ClusterName}-policy\\"\\n        PolicyDocument:\\n          Version: \\"2012-10-17\\"\\n          Statement:\\n          - Sid: AllowAssumeOrganizationAccountRole\\n            Effect: Allow\\n            Action: sts:AssumeRole\\n            Resource: !GetAtt RoleMyUser1.Arn\\n  AccessKeyMyUser1:\\n    Type: AWS::IAM::AccessKey\\n    Properties:\\n      UserName: !Ref UserMyUser1\\n  RoleMyUser1:\\n    Type: AWS::IAM::Role\\n    Properties:\\n      Description: !Sub \\"IAM role for the myuser1-${ClusterName} user\\"\\n      RoleName: !Sub \\"myuser1-${ClusterName}\\"\\n      AssumeRolePolicyDocument:\\n        Version: 2012-10-17\\n        Statement:\\n        - Effect: Allow\\n          Principal:\\n            AWS: !Sub \\"arn:aws:iam::${AWS::AccountId}:root\\"\\n          Action: sts:AssumeRole\\n  UserMyUser2:\\n    Type: AWS::IAM::User\\n    Properties:\\n      UserName: !Sub \\"myuser2-${ClusterName}\\"\\n      Policies:\\n      - PolicyName: !Sub \\"myuser2-${ClusterName}-policy\\"\\n        PolicyDocument:\\n          Version: \\"2012-10-17\\"\\n          Statement:\\n          - Sid: AllowAssumeOrganizationAccountRole\\n            Effect: Allow\\n            Action: sts:AssumeRole\\n            Resource: !GetAtt RoleMyUser2.Arn\\n  AccessKeyMyUser2:\\n    Type: AWS::IAM::AccessKey\\n    Properties:\\n      UserName: !Ref UserMyUser2\\n  RoleMyUser2:\\n    Type: AWS::IAM::Role\\n    Properties:\\n      Description: !Sub \\"IAM role for the myuser2-${ClusterName} user\\"\\n      RoleName: !Sub \\"myuser2-${ClusterName}\\"\\n      AssumeRolePolicyDocument:\\n        Version: 2012-10-17\\n        Statement:\\n        - Effect: Allow\\n          Principal:\\n            AWS: !Sub \\"arn:aws:iam::${AWS::AccountId}:root\\"\\n          Action: sts:AssumeRole\\nOutputs:\\n  CloudWatchPolicyArn:\\n    Description: The ARN of the created CloudWatchPolicy\\n    Value: !Ref CloudWatchPolicy\\n    Export:\\n      Name:\\n        Fn::Sub: \\"${AWS::StackName}-CloudWatchPolicyArn\\"\\n  KMSKeyArn:\\n    Description: The ARN of the created KMS Key to encrypt EKS related services\\n    Value: !GetAtt KMSKey.Arn\\n    Export:\\n      Name:\\n        Fn::Sub: \\"${AWS::StackName}-KMSKeyArn\\"\\n  KMSKeyId:\\n    Description: The ID of the created KMS Key to encrypt EKS related services\\n    Value: !Ref KMSKey\\n    Export:\\n      Name:\\n        Fn::Sub: \\"${AWS::StackName}-KMSKeyId\\"\\n  HostedZoneArn:\\n    Description: The ARN of the created Route53 Zone for K8s cluster\\n    Value: !Ref HostedZone\\n    Export:\\n      Name:\\n        Fn::Sub: \\"${AWS::StackName}-HostedZoneArn\\"\\n  S3PolicyArn:\\n    Description: The ARN of the created AmazonS3 policy\\n    Value: !Ref S3Policy\\n    Export:\\n      Name:\\n        Fn::Sub: \\"${AWS::StackName}-S3PolicyArn\\"\\n  RoleMyUser1Arn:\\n    Description: The ARN of the MyUser1 IAM Role\\n    Value: !GetAtt RoleMyUser1.Arn\\n    Export:\\n      Name:\\n        Fn::Sub: \\"${AWS::StackName}-RoleMyUser1Arn\\"\\n  AccessKeyMyUser1:\\n    Description: The AccessKey for MyUser1 user\\n    Value: !Ref AccessKeyMyUser1\\n    Export:\\n      Name:\\n        Fn::Sub: \\"${AWS::StackName}-AccessKeyMyUser1\\"\\n  SecretAccessKeyMyUser1:\\n    Description: The SecretAccessKey for MyUser1 user\\n    Value: !GetAtt AccessKeyMyUser1.SecretAccessKey\\n    Export:\\n      Name:\\n        Fn::Sub: \\"${AWS::StackName}-SecretAccessKeyMyUser1\\"\\n  RoleMyUser2Arn:\\n    Description: The ARN of the MyUser2 IAM Role\\n    Value: !GetAtt RoleMyUser2.Arn\\n    Export:\\n      Name:\\n        Fn::Sub: \\"${AWS::StackName}-RoleMyUser2Arn\\"\\n  AccessKeyMyUser2:\\n    Description: The AccessKey for MyUser2 user\\n    Value: !Ref AccessKeyMyUser2\\n    Export:\\n      Name:\\n        Fn::Sub: \\"${AWS::StackName}-AccessKeyMyUser2\\"\\n  SecretAccessKeyMyUser2:\\n    Description: The SecretAccessKey for MyUser2 user\\n    Value: !GetAtt AccessKeyMyUser2.SecretAccessKey\\n    Export:\\n      Name:\\n        Fn::Sub: \\"${AWS::StackName}-SecretAccessKeyMyUser2\\"\\nEOF\\n\\neval aws cloudformation deploy --capabilities CAPABILITY_NAMED_IAM \\\\\\n  --parameter-overrides \\"ClusterFQDN=${CLUSTER_FQDN} ClusterName=${CLUSTER_NAME} BaseDomain=${BASE_DOMAIN}\\" \\\\\\n  --stack-name \\"${CLUSTER_NAME}-route53-iam-s3-kms-asm\\" --template-file \\"tmp/${CLUSTER_FQDN}/aws-route53-iam-s3-kms-asm.yml\\" --tags \\"${TAGS}\\"\\n\\nAWS_CLOUDFORMATION_DETAILS=$(aws cloudformation describe-stacks --stack-name \\"${CLUSTER_NAME}-route53-iam-s3-kms-asm\\")\\n# CLOUDWATCH_POLICY_ARN=$(echo \\"${AWS_CLOUDFORMATION_DETAILS}\\" | jq -r \\".Stacks[0].Outputs[] | select(.OutputKey==\\\\\\"CloudWatchPolicyArn\\\\\\") .OutputValue\\")\\nKMS_KEY_ARN=$(echo \\"${AWS_CLOUDFORMATION_DETAILS}\\" | jq -r \\".Stacks[0].Outputs[] | select(.OutputKey==\\\\\\"KMSKeyArn\\\\\\") .OutputValue\\")\\nKMS_KEY_ID=$(echo \\"${AWS_CLOUDFORMATION_DETAILS}\\" | jq -r \\".Stacks[0].Outputs[] | select(.OutputKey==\\\\\\"KMSKeyId\\\\\\") .OutputValue\\")\\nS3_POLICY_ARN=$(echo \\"${AWS_CLOUDFORMATION_DETAILS}\\" | jq -r \\".Stacks[0].Outputs[] | select(.OutputKey==\\\\\\"S3PolicyArn\\\\\\") .OutputValue\\")\\n# MYUSER1_ROLE_ARN=$(echo \\"${AWS_CLOUDFORMATION_DETAILS}\\" | jq -r \\".Stacks[0].Outputs[] | select(.OutputKey==\\\\\\"RoleMyUser1Arn\\\\\\") .OutputValue\\")\\n# MYUSER1_USER_ACCESSKEYMYUSER=$(echo \\"${AWS_CLOUDFORMATION_DETAILS}\\" | jq -r \\".Stacks[0].Outputs[] | select(.OutputKey==\\\\\\"AccessKeyMyUser1\\\\\\") .OutputValue\\")\\n# MYUSER1_USER_SECRETACCESSKEY=$(echo \\"${AWS_CLOUDFORMATION_DETAILS}\\" | jq -r \\".Stacks[0].Outputs[] | select(.OutputKey==\\\\\\"SecretAccessKeyMyUser1\\\\\\") .OutputValue\\")\\n# MYUSER2_ROLE_ARN=$(echo \\"${AWS_CLOUDFORMATION_DETAILS}\\" | jq -r \\".Stacks[0].Outputs[] | select(.OutputKey==\\\\\\"RoleMyUser2Arn\\\\\\") .OutputValue\\")\\n# MYUSER2_USER_ACCESSKEYMYUSER=$(echo \\"${AWS_CLOUDFORMATION_DETAILS}\\" | jq -r \\".Stacks[0].Outputs[] | select(.OutputKey==\\\\\\"AccessKeyMyUser2\\\\\\") .OutputValue\\")\\n# MYUSER2_USER_SECRETACCESSKEY=$(echo \\"${AWS_CLOUDFORMATION_DETAILS}\\" | jq -r \\".Stacks[0].Outputs[] | select(.OutputKey==\\\\\\"SecretAccessKeyMyUser2\\\\\\") .OutputValue\\")\\n```\\n\\nChange TTL=60 of SOA + NS records for new domain\\n(it can not be done in CloudFormation):\\n\\n```bash\\nHOSTED_ZONE_ID=$(aws route53 list-hosted-zones --query \\"HostedZones[?Name==\\\\`${CLUSTER_FQDN}.\\\\`].Id\\" --output text)\\nRESOURCE_RECORD_SET_SOA=$(aws route53 --output json list-resource-record-sets --hosted-zone-id \\"${HOSTED_ZONE_ID}\\" --query \\"(ResourceRecordSets[?Type == \\\\`SOA\\\\`])[0]\\" | sed \\"s/\\\\\\"TTL\\\\\\":.*/\\\\\\"TTL\\\\\\": 60,/\\")\\nRESOURCE_RECORD_SET_NS=$(aws route53 --output json list-resource-record-sets --hosted-zone-id \\"${HOSTED_ZONE_ID}\\" --query \\"(ResourceRecordSets[?Type == \\\\`NS\\\\`])[0]\\" | sed \\"s/\\\\\\"TTL\\\\\\":.*/\\\\\\"TTL\\\\\\": 60,/\\")\\ncat << EOF | aws route53 --output json change-resource-record-sets --hosted-zone-id \\"${HOSTED_ZONE_ID}\\" --change-batch=file:///dev/stdin\\n{\\n    \\"Comment\\": \\"Update record to reflect new TTL for SOA and NS records\\",\\n    \\"Changes\\": [\\n        {\\n            \\"Action\\": \\"UPSERT\\",\\n            \\"ResourceRecordSet\\":\\n${RESOURCE_RECORD_SET_SOA}\\n        },\\n        {\\n            \\"Action\\": \\"UPSERT\\",\\n            \\"ResourceRecordSet\\":\\n${RESOURCE_RECORD_SET_NS}\\n        }\\n    ]\\n}\\nEOF\\n```\\n\\n## Create Amazon EKS\\n\\n![EKS](https://raw.githubusercontent.com/aws-samples/eks-workshop/65b766c494a5b4f5420b2912d8373c4957163541/static/images/3-service-animated.gif\\n\\"EKS\\")\\n\\nCreate [Amazon EKS](https://aws.amazon.com/eks/) in AWS by using [eksctl](https://eksctl.io/).\\nIt\'s a tool from Weaveworks based on official\\nAWS CloudFormation templates which will be used to launch and configure our\\nEKS cluster and nodes.\\n\\n![eksctl](https://raw.githubusercontent.com/weaveworks/eksctl/c365149fc1a0b8d357139cbd6cda5aee8841c16c/logo/eksctl.png\\n\\"eksctl\\")\\n\\nGenerate SSH key if not exists:\\n\\n```bash\\ntest -f ~/.ssh/id_rsa.pub || (install -m 0700 -d ~/.ssh && ssh-keygen -b 2048 -t rsa -f ~/.ssh/id_rsa -q -N \\"\\")\\n```\\n\\nCreate the Amazon EKS cluster with Calico using `eksctl`:\\n\\n```bash\\ncat > \\"tmp/${CLUSTER_FQDN}/eksctl.yaml\\" << EOF\\napiVersion: eksctl.io/v1alpha5\\nkind: ClusterConfig\\nmetadata:\\n  name: ${CLUSTER_NAME}\\n  region: ${AWS_DEFAULT_REGION}\\n  version: \\"1.21\\"\\n  tags: &tags\\n$(echo \\"${TAGS}\\" | sed \\"s/ /\\\\\\\\n    /g; s/^/    /g; s/=/: /g\\")\\navailabilityZones:\\n  - ${AWS_DEFAULT_REGION}a\\n  - ${AWS_DEFAULT_REGION}b\\niam:\\n  withOIDC: true\\n  serviceAccounts:\\n    - metadata:\\n        name: aws-load-balancer-controller\\n        namespace: kube-system\\n      wellKnownPolicies:\\n        awsLoadBalancerController: true\\n    - metadata:\\n        name: cert-manager\\n        namespace: cert-manager\\n      wellKnownPolicies:\\n        certManager: true\\n    - metadata:\\n        name: cluster-autoscaler\\n        namespace: kube-system\\n      wellKnownPolicies:\\n        autoScaler: true\\n    - metadata:\\n        name: external-dns\\n        namespace: external-dns\\n      wellKnownPolicies:\\n        externalDNS: true\\n    - metadata:\\n        name: ebs-csi-controller-sa\\n        namespace: kube-system\\n      wellKnownPolicies:\\n        ebsCSIController: true\\n    - metadata:\\n        name: harbor\\n        namespace: harbor\\n      attachPolicyARNs:\\n        - ${S3_POLICY_ARN}\\n    - metadata:\\n        name: velero\\n        namespace: velero\\n      attachPolicyARNs:\\n        - ${S3_POLICY_ARN}\\n    - metadata:\\n        name: s3-test\\n        namespace: s3-test\\n      attachPolicyARNs:\\n        - ${S3_POLICY_ARN}\\n    - metadata:\\n        name: grafana\\n        namespace: kube-prometheus-stack\\n      attachPolicyARNs:\\n        - arn:aws:iam::aws:policy/AmazonPrometheusQueryAccess\\n        - arn:aws:iam::aws:policy/CloudWatchReadOnlyAccess\\n      attachPolicy:\\n        Version: 2012-10-17\\n        Statement:\\n        - Sid: AllowReadingTagsInstancesRegionsFromEC2\\n          Effect: Allow\\n          Action:\\n          - ec2:DescribeTags\\n          - ec2:DescribeInstances\\n          - ec2:DescribeRegions\\n          Resource: \\"*\\"\\n        - Sid: AllowReadingResourcesForTags\\n          Effect: Allow\\n          Action: tag:GetResources\\n          Resource: \\"*\\"\\n    - metadata:\\n        name: kube-prometheus-stack-prometheus\\n        namespace: kube-prometheus-stack\\n      attachPolicyARNs:\\n        - arn:aws:iam::aws:policy/AmazonPrometheusQueryAccess\\n        - arn:aws:iam::aws:policy/AmazonPrometheusRemoteWriteAccess\\n    - metadata:\\n        name: efs-csi-controller-sa\\n        namespace: kube-system\\n      wellKnownPolicies:\\n        efsCSIController: true\\n    - metadata:\\n        name: vault\\n        namespace: vault\\n      attachPolicy:\\n        Version: 2012-10-17\\n        Statement:\\n        - Sid: VaultKMSUnseal\\n          Effect: Allow\\n          Action:\\n          - kms:Encrypt\\n          - kms:Decrypt\\n          - kms:DescribeKey\\n          Resource:\\n          - \\"${KMS_KEY_ARN}\\"\\n    - metadata:\\n        name: kuard\\n        namespace: kuard\\n      attachPolicy:\\n        Version: 2012-10-17\\n        Statement:\\n        - Sid: AllowSecretManagerAccess\\n          Effect: Allow\\n          Action:\\n          - secretsmanager:GetSecretValue\\n          - secretsmanager:DescribeSecret\\n          Resource:\\n          - \\"arn:aws:secretsmanager:*:*:secret:*\\"\\n        - Sid: AllowKMSAccess\\n          Effect: Allow\\n          Action:\\n          - kms:Decrypt\\n          Resource:\\n          - \\"${KMS_KEY_ARN}\\"\\nvpc:\\n  nat:\\n    gateway: Disable\\nmanagedNodeGroups:\\n  - name: managed-ng-1\\n    amiFamily: Bottlerocket\\n    instanceType: t3.xlarge\\n    instancePrefix: ruzickap\\n    desiredCapacity: 3\\n    minSize: 2\\n    maxSize: 5\\n    volumeSize: 30\\n    labels:\\n      role: worker\\n    tags: *tags\\n    iam:\\n      withAddonPolicies:\\n        autoScaler: true\\n        cloudWatch: true\\n        ebs: true\\n        efs: true\\n    maxPodsPerNode: 1000\\n    volumeEncrypted: true\\n    volumeKmsKeyID: ${KMS_KEY_ID}\\nfargateProfiles:\\n  - name: fp-fgtest\\n    selectors:\\n      - namespace: fgtest\\n    tags: *tags\\nsecretsEncryption:\\n  keyARN: ${KMS_KEY_ARN}\\ncloudWatch:\\n  clusterLogging:\\n    enableTypes:\\n      - authenticator\\nEOF\\n\\nif ! eksctl get clusters --name=\\"${CLUSTER_NAME}\\" &> /dev/null; then\\n  eksctl create cluster --config-file \\"tmp/${CLUSTER_FQDN}/eksctl.yaml\\" --kubeconfig \\"${KUBECONFIG}\\" --without-nodegroup\\n  kubectl delete daemonset -n kube-system aws-node\\n  kubectl apply -f https://docs.projectcalico.org/archive/v3.20/manifests/calico-vxlan.yaml\\n  eksctl create nodegroup --config-file \\"tmp/${CLUSTER_FQDN}/eksctl.yaml\\"\\nfi\\n```\\n\\nOutput:\\n\\n```text\\n2021-11-29 17:52:50 [\u2139]  eksctl version 0.75.0\\n2021-11-29 17:52:50 [\u2139]  using region eu-west-1\\n2021-11-29 17:52:50 [\u2139]  subnets for eu-west-1a - public:192.168.0.0/19 private:192.168.64.0/19\\n2021-11-29 17:52:50 [\u2139]  subnets for eu-west-1b - public:192.168.32.0/19 private:192.168.96.0/19\\n2021-11-29 17:52:50 [\u2139]  using Kubernetes version 1.21\\n2021-11-29 17:52:50 [\u2139]  creating EKS cluster \\"kube1\\" in \\"eu-west-1\\" region with Fargate profile\\n2021-11-29 17:52:50 [\u2139]  will create a CloudFormation stack for cluster itself and 0 nodegroup stack(s)\\n2021-11-29 17:52:50 [\u2139]  will create a CloudFormation stack for cluster itself and 0 managed nodegroup stack(s)\\n2021-11-29 17:52:50 [\u2139]  if you encounter any issues, check CloudFormation console or try \'eksctl utils describe-stacks --region=eu-west-1 --cluster=kube1\'\\n2021-11-29 17:52:50 [\u2139]  Kubernetes API endpoint access will use default of {publicAccess=true, privateAccess=false} for cluster \\"kube1\\" in \\"eu-west-1\\"\\n2021-11-29 17:52:50 [\u2139]\\n2 sequential tasks: { create cluster control plane \\"kube1\\",\\n    7 sequential sub-tasks: {\\n        wait for control plane to become ready,\\n        tag cluster,\\n        update CloudWatch logging configuration,\\n        create fargate profiles,\\n        associate IAM OIDC provider,\\n        14 parallel sub-tasks: {\\n            2 sequential sub-tasks: {\\n                create IAM role for serviceaccount \\"kube-system/aws-load-balancer-controller\\",\\n                create serviceaccount \\"kube-system/aws-load-balancer-controller\\",\\n            },\\n            2 sequential sub-tasks: {\\n                create IAM role for serviceaccount \\"cert-manager/cert-manager\\",\\n                create serviceaccount \\"cert-manager/cert-manager\\",\\n            },\\n            2 sequential sub-tasks: {\\n                create IAM role for serviceaccount \\"kube-system/cluster-autoscaler\\",\\n                create serviceaccount \\"kube-system/cluster-autoscaler\\",\\n            },\\n            2 sequential sub-tasks: {\\n                create IAM role for serviceaccount \\"external-dns/external-dns\\",\\n                create serviceaccount \\"external-dns/external-dns\\",\\n            },\\n            2 sequential sub-tasks: {\\n                create IAM role for serviceaccount \\"kube-system/ebs-csi-controller-sa\\",\\n                create serviceaccount \\"kube-system/ebs-csi-controller-sa\\",\\n            },\\n            2 sequential sub-tasks: {\\n                create IAM role for serviceaccount \\"harbor/harbor\\",\\n                create serviceaccount \\"harbor/harbor\\",\\n            },\\n            2 sequential sub-tasks: {\\n                create IAM role for serviceaccount \\"velero/velero\\",\\n                create serviceaccount \\"velero/velero\\",\\n            },\\n            2 sequential sub-tasks: {\\n                create IAM role for serviceaccount \\"s3-test/s3-test\\",\\n                create serviceaccount \\"s3-test/s3-test\\",\\n            },\\n            2 sequential sub-tasks: {\\n                create IAM role for serviceaccount \\"kube-prometheus-stack/grafana\\",\\n                create serviceaccount \\"kube-prometheus-stack/grafana\\",\\n            },\\n            2 sequential sub-tasks: {\\n                create IAM role for serviceaccount \\"kube-prometheus-stack/kube-prometheus-stack-prometheus\\",\\n                create serviceaccount \\"kube-prometheus-stack/kube-prometheus-stack-prometheus\\",\\n            },\\n            2 sequential sub-tasks: {\\n                create IAM role for serviceaccount \\"kube-system/efs-csi-controller-sa\\",\\n                create serviceaccount \\"kube-system/efs-csi-controller-sa\\",\\n            },\\n            2 sequential sub-tasks: {\\n                create IAM role for serviceaccount \\"vault/vault\\",\\n                create serviceaccount \\"vault/vault\\",\\n            },\\n            2 sequential sub-tasks: {\\n                create IAM role for serviceaccount \\"kuard/kuard\\",\\n                create serviceaccount \\"kuard/kuard\\",\\n            },\\n            2 sequential sub-tasks: {\\n                create IAM role for serviceaccount \\"kube-system/aws-node\\",\\n                create serviceaccount \\"kube-system/aws-node\\",\\n            },\\n        },\\n        restart daemonset \\"kube-system/aws-node\\",\\n    }\\n}\\n2021-11-29 17:52:50 [\u2139]  building cluster stack \\"eksctl-kube1-cluster\\"\\n2021-11-29 17:52:50 [\u2139]  deploying stack \\"eksctl-kube1-cluster\\"\\n2021-11-29 17:53:21 [\u2139]  waiting for CloudFormation stack \\"eksctl-kube1-cluster\\"\\n...\\n2021-11-29 18:05:55 [\u2139]  waiting for CloudFormation stack \\"eksctl-kube1-cluster\\"\\n2021-11-29 18:07:59 [\u2714]  tagged EKS cluster (Owner=petr.ruzicka@gmail.com, Squad=Cloud_Container_Platform, compliance:na:defender=bottlerocket, Environment=Dev, Group=Cloud_Native)\\n2021-11-29 18:08:00 [\u2139]  waiting for requested \\"LoggingUpdate\\" in cluster \\"kube1\\" to succeed\\n...\\n2021-11-29 18:08:53 [\u2139]  waiting for requested \\"LoggingUpdate\\" in cluster \\"kube1\\" to succeed\\n2021-11-29 18:08:54 [\u2714]  configured CloudWatch logging for cluster \\"kube1\\" in \\"eu-west-1\\" (enabled types: authenticator & disabled types: api, audit, controllerManager, scheduler)\\n2021-11-29 18:08:54 [\u2139]  creating Fargate profile \\"fp-fgtest\\" on EKS cluster \\"kube1\\"\\n2021-11-29 18:13:12 [\u2139]  created Fargate profile \\"fp-fgtest\\" on EKS cluster \\"kube1\\"\\n2021-11-29 18:17:44 [\u2139]  building iamserviceaccount stack \\"eksctl-kube1-addon-iamserviceaccount-kube-system-aws-load-balancer-controller\\"\\n2021-11-29 18:17:44 [\u2139]  building iamserviceaccount stack \\"eksctl-kube1-addon-iamserviceaccount-kube-prometheus-stack-grafana\\"\\n2021-11-29 18:17:44 [\u2139]  building iamserviceaccount stack \\"eksctl-kube1-addon-iamserviceaccount-velero-velero\\"\\n2021-11-29 18:17:44 [\u2139]  building iamserviceaccount stack \\"eksctl-kube1-addon-iamserviceaccount-kube-system-efs-csi-controller-sa\\"\\n2021-11-29 18:17:44 [\u2139]  building iamserviceaccount stack \\"eksctl-kube1-addon-iamserviceaccount-kuard-kuard\\"\\n2021-11-29 18:17:44 [\u2139]  building iamserviceaccount stack \\"eksctl-kube1-addon-iamserviceaccount-cert-manager-cert-manager\\"\\n2021-11-29 18:17:44 [\u2139]  building iamserviceaccount stack \\"eksctl-kube1-addon-iamserviceaccount-kube-system-aws-node\\"\\n2021-11-29 18:17:44 [\u2139]  building iamserviceaccount stack \\"eksctl-kube1-addon-iamserviceaccount-kube-system-ebs-csi-controller-sa\\"\\n2021-11-29 18:17:44 [\u2139]  building iamserviceaccount stack \\"eksctl-kube1-addon-iamserviceaccount-kube-system-cluster-autoscaler\\"\\n2021-11-29 18:17:44 [\u2139]  building iamserviceaccount stack \\"eksctl-kube1-addon-iamserviceaccount-kube-prometheus-stack-kube-prometheus-stack-prometheus\\"\\n2021-11-29 18:17:44 [\u2139]  building iamserviceaccount stack \\"eksctl-kube1-addon-iamserviceaccount-external-dns-external-dns\\"\\n2021-11-29 18:17:44 [\u2139]  building iamserviceaccount stack \\"eksctl-kube1-addon-iamserviceaccount-s3-test-s3-test\\"\\n2021-11-29 18:17:44 [\u2139]  building iamserviceaccount stack \\"eksctl-kube1-addon-iamserviceaccount-vault-vault\\"\\n2021-11-29 18:17:44 [\u2139]  building iamserviceaccount stack \\"eksctl-kube1-addon-iamserviceaccount-harbor-harbor\\"\\n2021-11-29 18:17:45 [\u2139]  deploying stack \\"eksctl-kube1-addon-iamserviceaccount-kube-system-cluster-autoscaler\\"\\n2021-11-29 18:17:45 [\u2139]  waiting for CloudFormation stack \\"eksctl-kube1-addon-iamserviceaccount-kube-system-cluster-autoscaler\\"\\n2021-11-29 18:17:45 [\u2139]  deploying stack \\"eksctl-kube1-addon-iamserviceaccount-harbor-harbor\\"\\n2021-11-29 18:17:45 [\u2139]  waiting for CloudFormation stack \\"eksctl-kube1-addon-iamserviceaccount-harbor-harbor\\"\\n2021-11-29 18:17:45 [\u2139]  deploying stack \\"eksctl-kube1-addon-iamserviceaccount-kube-prometheus-stack-grafana\\"\\n2021-11-29 18:17:45 [\u2139]  waiting for CloudFormation stack \\"eksctl-kube1-addon-iamserviceaccount-kube-prometheus-stack-grafana\\"\\n2021-11-29 18:17:45 [\u2139]  deploying stack \\"eksctl-kube1-addon-iamserviceaccount-s3-test-s3-test\\"\\n2021-11-29 18:17:45 [\u2139]  waiting for CloudFormation stack \\"eksctl-kube1-addon-iamserviceaccount-s3-test-s3-test\\"\\n2021-11-29 18:17:45 [\u2139]  deploying stack \\"eksctl-kube1-addon-iamserviceaccount-cert-manager-cert-manager\\"\\n2021-11-29 18:17:45 [\u2139]  waiting for CloudFormation stack \\"eksctl-kube1-addon-iamserviceaccount-cert-manager-cert-manager\\"\\n2021-11-29 18:17:45 [\u2139]  deploying stack \\"eksctl-kube1-addon-iamserviceaccount-kube-system-efs-csi-controller-sa\\"\\n2021-11-29 18:17:45 [\u2139]  deploying stack \\"eksctl-kube1-addon-iamserviceaccount-vault-vault\\"\\n2021-11-29 18:17:45 [\u2139]  waiting for CloudFormation stack \\"eksctl-kube1-addon-iamserviceaccount-kube-system-efs-csi-controller-sa\\"\\n2021-11-29 18:17:45 [\u2139]  waiting for CloudFormation stack \\"eksctl-kube1-addon-iamserviceaccount-vault-vault\\"\\n2021-11-29 18:17:45 [\u2139]  deploying stack \\"eksctl-kube1-addon-iamserviceaccount-velero-velero\\"\\n2021-11-29 18:17:45 [\u2139]  waiting for CloudFormation stack \\"eksctl-kube1-addon-iamserviceaccount-velero-velero\\"\\n2021-11-29 18:17:45 [\u2139]  deploying stack \\"eksctl-kube1-addon-iamserviceaccount-kuard-kuard\\"\\n2021-11-29 18:17:45 [\u2139]  waiting for CloudFormation stack \\"eksctl-kube1-addon-iamserviceaccount-kuard-kuard\\"\\n2021-11-29 18:17:45 [\u2139]  deploying stack \\"eksctl-kube1-addon-iamserviceaccount-external-dns-external-dns\\"\\n2021-11-29 18:17:45 [\u2139]  waiting for CloudFormation stack \\"eksctl-kube1-addon-iamserviceaccount-external-dns-external-dns\\"\\n2021-11-29 18:17:45 [\u2139]  deploying stack \\"eksctl-kube1-addon-iamserviceaccount-kube-prometheus-stack-kube-prometheus-stack-prometheus\\"\\n2021-11-29 18:17:45 [\u2139]  waiting for CloudFormation stack \\"eksctl-kube1-addon-iamserviceaccount-kube-prometheus-stack-kube-prometheus-stack-prometheus\\"\\n2021-11-29 18:17:45 [\u2139]  deploying stack \\"eksctl-kube1-addon-iamserviceaccount-kube-system-aws-node\\"\\n2021-11-29 18:17:45 [\u2139]  waiting for CloudFormation stack \\"eksctl-kube1-addon-iamserviceaccount-kube-system-aws-node\\"\\n2021-11-29 18:17:45 [\u2139]  deploying stack \\"eksctl-kube1-addon-iamserviceaccount-kube-system-aws-load-balancer-controller\\"\\n2021-11-29 18:17:45 [\u2139]  waiting for CloudFormation stack \\"eksctl-kube1-addon-iamserviceaccount-kube-system-aws-load-balancer-controller\\"\\n2021-11-29 18:17:45 [\u2139]  deploying stack \\"eksctl-kube1-addon-iamserviceaccount-kube-system-ebs-csi-controller-sa\\"\\n2021-11-29 18:17:45 [\u2139]  waiting for CloudFormation stack \\"eksctl-kube1-addon-iamserviceaccount-kube-system-ebs-csi-controller-sa\\"\\n2021-11-29 18:18:00 [\u2139]  waiting for CloudFormation stack \\"eksctl-kube1-addon-iamserviceaccount-kube-system-cluster-autoscaler\\"\\n2021-11-29 18:18:00 [\u2139]  waiting for CloudFormation stack \\"eksctl-kube1-addon-iamserviceaccount-kube-prometheus-stack-grafana\\"\\n2021-11-29 18:18:01 [\u2139]  waiting for CloudFormation stack \\"eksctl-kube1-addon-iamserviceaccount-velero-velero\\"\\n2021-11-29 18:18:02 [\u2139]  waiting for CloudFormation stack \\"eksctl-kube1-addon-iamserviceaccount-kube-system-aws-load-balancer-controller\\"\\n2021-11-29 18:18:02 [\u2139]  waiting for CloudFormation stack \\"eksctl-kube1-addon-iamserviceaccount-harbor-harbor\\"\\n2021-11-29 18:18:02 [\u2139]  waiting for CloudFormation stack \\"eksctl-kube1-addon-iamserviceaccount-s3-test-s3-test\\"\\n2021-11-29 18:18:02 [\u2139]  waiting for CloudFormation stack \\"eksctl-kube1-addon-iamserviceaccount-kube-system-ebs-csi-controller-sa\\"\\n2021-11-29 18:18:03 [\u2139]  waiting for CloudFormation stack \\"eksctl-kube1-addon-iamserviceaccount-cert-manager-cert-manager\\"\\n2021-11-29 18:18:03 [\u2139]  waiting for CloudFormation stack \\"eksctl-kube1-addon-iamserviceaccount-vault-vault\\"\\n2021-11-29 18:18:03 [\u2139]  waiting for CloudFormation stack \\"eksctl-kube1-addon-iamserviceaccount-kuard-kuard\\"\\n2021-11-29 18:18:04 [\u2139]  waiting for CloudFormation stack \\"eksctl-kube1-addon-iamserviceaccount-kube-system-efs-csi-controller-sa\\"\\n2021-11-29 18:18:04 [\u2139]  waiting for CloudFormation stack \\"eksctl-kube1-addon-iamserviceaccount-kube-system-aws-node\\"\\n2021-11-29 18:18:04 [\u2139]  waiting for CloudFormation stack \\"eksctl-kube1-addon-iamserviceaccount-kube-prometheus-stack-kube-prometheus-stack-prometheus\\"\\n2021-11-29 18:18:05 [\u2139]  waiting for CloudFormation stack \\"eksctl-kube1-addon-iamserviceaccount-external-dns-external-dns\\"\\n2021-11-29 18:18:17 [\u2139]  waiting for CloudFormation stack \\"eksctl-kube1-addon-iamserviceaccount-harbor-harbor\\"\\n2021-11-29 18:18:18 [\u2139]  created namespace \\"harbor\\"\\n2021-11-29 18:18:18 [\u2139]  created serviceaccount \\"harbor/harbor\\"\\n2021-11-29 18:18:18 [\u2139]  waiting for CloudFormation stack \\"eksctl-kube1-addon-iamserviceaccount-s3-test-s3-test\\"\\n2021-11-29 18:18:18 [\u2139]  waiting for CloudFormation stack \\"eksctl-kube1-addon-iamserviceaccount-kube-system-cluster-autoscaler\\"\\n2021-11-29 18:18:18 [\u2139]  created namespace \\"s3-test\\"\\n2021-11-29 18:18:18 [\u2139]  created serviceaccount \\"s3-test/s3-test\\"\\n2021-11-29 18:18:19 [\u2139]  waiting for CloudFormation stack \\"eksctl-kube1-addon-iamserviceaccount-velero-velero\\"\\n2021-11-29 18:18:19 [\u2139]  waiting for CloudFormation stack \\"eksctl-kube1-addon-iamserviceaccount-cert-manager-cert-manager\\"\\n2021-11-29 18:18:19 [\u2139]  created namespace \\"velero\\"\\n2021-11-29 18:18:19 [\u2139]  waiting for CloudFormation stack \\"eksctl-kube1-addon-iamserviceaccount-kube-system-aws-load-balancer-controller\\"\\n2021-11-29 18:18:20 [\u2139]  created serviceaccount \\"velero/velero\\"\\n2021-11-29 18:18:20 [\u2139]  waiting for CloudFormation stack \\"eksctl-kube1-addon-iamserviceaccount-vault-vault\\"\\n2021-11-29 18:18:20 [\u2139]  waiting for CloudFormation stack \\"eksctl-kube1-addon-iamserviceaccount-kube-prometheus-stack-grafana\\"\\n2021-11-29 18:18:21 [\u2139]  waiting for CloudFormation stack \\"eksctl-kube1-addon-iamserviceaccount-kube-prometheus-stack-kube-prometheus-stack-prometheus\\"\\n2021-11-29 18:18:21 [\u2139]  created namespace \\"kube-prometheus-stack\\"\\n2021-11-29 18:18:21 [\u2139]  created serviceaccount \\"kube-prometheus-stack/kube-prometheus-stack-prometheus\\"\\n2021-11-29 18:18:21 [\u2139]  waiting for CloudFormation stack \\"eksctl-kube1-addon-iamserviceaccount-kube-system-ebs-csi-controller-sa\\"\\n2021-11-29 18:18:21 [\u2139]  waiting for CloudFormation stack \\"eksctl-kube1-addon-iamserviceaccount-kuard-kuard\\"\\n2021-11-29 18:18:24 [\u2139]  waiting for CloudFormation stack \\"eksctl-kube1-addon-iamserviceaccount-kube-system-efs-csi-controller-sa\\"\\n2021-11-29 18:18:24 [\u2139]  waiting for CloudFormation stack \\"eksctl-kube1-addon-iamserviceaccount-external-dns-external-dns\\"\\n2021-11-29 18:18:24 [\u2139]  waiting for CloudFormation stack \\"eksctl-kube1-addon-iamserviceaccount-kube-system-aws-node\\"\\n2021-11-29 18:18:24 [\u2139]  serviceaccount \\"kube-system/aws-node\\" already exists\\n2021-11-29 18:18:24 [\u2139]  updated serviceaccount \\"kube-system/aws-node\\"\\n2021-11-29 18:18:35 [\u2139]  waiting for CloudFormation stack \\"eksctl-kube1-addon-iamserviceaccount-kube-system-cluster-autoscaler\\"\\n2021-11-29 18:18:36 [\u2139]  created serviceaccount \\"kube-system/cluster-autoscaler\\"\\n2021-11-29 18:18:37 [\u2139]  waiting for CloudFormation stack \\"eksctl-kube1-addon-iamserviceaccount-kube-prometheus-stack-grafana\\"\\n2021-11-29 18:18:38 [\u2139]  waiting for CloudFormation stack \\"eksctl-kube1-addon-iamserviceaccount-kuard-kuard\\"\\n2021-11-29 18:18:38 [\u2139]  created serviceaccount \\"kube-prometheus-stack/grafana\\"\\n2021-11-29 18:18:38 [\u2139]  created namespace \\"kuard\\"\\n2021-11-29 18:18:38 [\u2139]  created serviceaccount \\"kuard/kuard\\"\\n2021-11-29 18:18:38 [\u2139]  waiting for CloudFormation stack \\"eksctl-kube1-addon-iamserviceaccount-vault-vault\\"\\n2021-11-29 18:18:38 [\u2139]  waiting for CloudFormation stack \\"eksctl-kube1-addon-iamserviceaccount-cert-manager-cert-manager\\"\\n2021-11-29 18:18:38 [\u2139]  waiting for CloudFormation stack \\"eksctl-kube1-addon-iamserviceaccount-kube-system-aws-load-balancer-controller\\"\\n2021-11-29 18:18:38 [\u2139]  waiting for CloudFormation stack \\"eksctl-kube1-addon-iamserviceaccount-kube-system-ebs-csi-controller-sa\\"\\n2021-11-29 18:18:38 [\u2139]  created namespace \\"vault\\"\\n2021-11-29 18:18:39 [\u2139]  created serviceaccount \\"vault/vault\\"\\n2021-11-29 18:18:39 [\u2139]  created namespace \\"cert-manager\\"\\n2021-11-29 18:18:39 [\u2139]  created serviceaccount \\"cert-manager/cert-manager\\"\\n2021-11-29 18:18:39 [\u2139]  created serviceaccount \\"kube-system/aws-load-balancer-controller\\"\\n2021-11-29 18:18:39 [\u2139]  created serviceaccount \\"kube-system/ebs-csi-controller-sa\\"\\n2021-11-29 18:18:41 [\u2139]  waiting for CloudFormation stack \\"eksctl-kube1-addon-iamserviceaccount-external-dns-external-dns\\"\\n2021-11-29 18:18:42 [\u2139]  created namespace \\"external-dns\\"\\n2021-11-29 18:18:42 [\u2139]  created serviceaccount \\"external-dns/external-dns\\"\\n2021-11-29 18:18:42 [\u2139]  waiting for CloudFormation stack \\"eksctl-kube1-addon-iamserviceaccount-kube-system-efs-csi-controller-sa\\"\\n2021-11-29 18:18:43 [\u2139]  created serviceaccount \\"kube-system/efs-csi-controller-sa\\"\\n2021-11-29 18:18:43 [\u2139]  daemonset \\"kube-system/aws-node\\" restarted\\n2021-11-29 18:18:43 [\u2139]  waiting for the control plane availability...\\n2021-11-29 18:18:43 [\u2714]  saved kubeconfig as \\"/Users/ruzickap/git/k8s-eks-bottlerocket-fargate/kubeconfig-kube1.conf\\"\\n2021-11-29 18:18:43 [\u2139]  no tasks\\n2021-11-29 18:18:43 [\u2714]  all EKS cluster resources for \\"kube1\\" have been created\\n2021-11-29 18:18:44 [\u2139]  kubectl command should work with \\"/Users/ruzickap/git/k8s-eks-bottlerocket-fargate/kubeconfig-kube1.conf\\", try \'kubectl --kubeconfig=/Users/ruzickap/git/k8s-eks-bottlerocket-fargate/kubeconfig-kube1.conf get nodes\'\\n2021-11-29 18:18:44 [\u2714]  EKS cluster \\"kube1\\" in \\"eu-west-1\\" region is ready\\ndaemonset.apps \\"aws-node\\" deleted\\nconfigmap/calico-config created\\ncustomresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created\\ncustomresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created\\ncustomresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created\\ncustomresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created\\ncustomresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created\\ncustomresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created\\ncustomresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created\\ncustomresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created\\ncustomresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created\\ncustomresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created\\ncustomresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created\\ncustomresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created\\ncustomresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org created\\ncustomresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created\\ncustomresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created\\nclusterrole.rbac.authorization.k8s.io/calico-kube-controllers created\\nclusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created\\nclusterrole.rbac.authorization.k8s.io/calico-node created\\nclusterrolebinding.rbac.authorization.k8s.io/calico-node created\\ndaemonset.apps/calico-node created\\nserviceaccount/calico-node created\\ndeployment.apps/calico-kube-controllers created\\nserviceaccount/calico-kube-controllers created\\nWarning: policy/v1beta1 PodDisruptionBudget is deprecated in v1.21+, unavailable in v1.25+; use policy/v1 PodDisruptionBudget\\npoddisruptionbudget.policy/calico-kube-controllers created\\n2021-11-29 18:18:59 [\u2139]  eksctl version 0.75.0\\n2021-11-29 18:18:59 [\u2139]  using region eu-west-1\\n2021-11-29 18:19:15 [\u2139]  nodegroup \\"managed-ng-1\\" will use \\"\\" [Bottlerocket/1.21]\\n2021-11-29 18:19:32 [\u2139]  1 nodegroup (managed-ng-1) was included (based on the include/exclude rules)\\n2021-11-29 18:19:32 [\u2139]  will create a CloudFormation stack for each of 1 managed nodegroups in cluster \\"kube1\\"\\n2021-11-29 18:19:32 [\u2139]\\n2 sequential tasks: { fix cluster compatibility, 1 task: { 1 task: { create managed nodegroup \\"managed-ng-1\\" } }\\n}\\n2021-11-29 18:19:32 [\u2139]  checking cluster stack for missing resources\\n2021-11-29 18:19:41 [\u2139]  cluster stack has all required resources\\n2021-11-29 18:19:41 [\u2139]  building managed nodegroup stack \\"eksctl-kube1-nodegroup-managed-ng-1\\"\\n2021-11-29 18:19:41 [\u2139]  deploying stack \\"eksctl-kube1-nodegroup-managed-ng-1\\"\\n2021-11-29 18:19:41 [\u2139]  waiting for CloudFormation stack \\"eksctl-kube1-nodegroup-managed-ng-1\\"\\n...\\n2021-11-29 18:22:59 [\u2139]  waiting for CloudFormation stack \\"eksctl-kube1-nodegroup-managed-ng-1\\"\\n2021-11-29 18:23:00 [\u2139]  no tasks\\n2021-11-29 18:23:00 [\u2714]  created 0 nodegroup(s) in cluster \\"kube1\\"\\n2021-11-29 18:23:00 [\u2139]  nodegroup \\"managed-ng-1\\" has 3 node(s)\\n2021-11-29 18:23:00 [\u2139]  node \\"ip-192-168-31-11.eu-west-1.compute.internal\\" is ready\\n2021-11-29 18:23:00 [\u2139]  node \\"ip-192-168-56-82.eu-west-1.compute.internal\\" is ready\\n2021-11-29 18:23:00 [\u2139]  node \\"ip-192-168-60-184.eu-west-1.compute.internal\\" is ready\\n2021-11-29 18:23:00 [\u2139]  waiting for at least 2 node(s) to become ready in \\"managed-ng-1\\"\\n2021-11-29 18:23:00 [\u2139]  nodegroup \\"managed-ng-1\\" has 3 node(s)\\n2021-11-29 18:23:00 [\u2139]  node \\"ip-192-168-31-11.eu-west-1.compute.internal\\" is ready\\n2021-11-29 18:23:00 [\u2139]  node \\"ip-192-168-56-82.eu-west-1.compute.internal\\" is ready\\n2021-11-29 18:23:00 [\u2139]  node \\"ip-192-168-60-184.eu-west-1.compute.internal\\" is ready\\n2021-11-29 18:23:00 [\u2714]  created 1 managed nodegroup(s) in cluster \\"kube1\\"\\n2021-11-29 18:23:12 [\u2139]  checking security group configuration for all nodegroups\\n2021-11-29 18:23:12 [\u2139]  all nodegroups have up-to-date cloudformation templates\\n```\\n\\nWhen the cluster is ready it immediately start pushing logs to CloudWatch under\\n`/aws/eks/kube1/cluster`.\\n\\nAdd add the user or role to the aws-auth ConfigMap. This is handy if you are\\nusing different user for cli operations and different user/role for accessing\\nthe AWS Console to see EKS Workloads in Cluster\'s tab.\\n\\n```bash\\nif ! eksctl get iamidentitymapping --cluster=\\"${CLUSTER_NAME}\\" --region=\\"${AWS_DEFAULT_REGION}\\" --arn=${AWS_CONSOLE_ADMIN_ROLE_ARN}; then\\n  eksctl create iamidentitymapping --cluster=\\"${CLUSTER_NAME}\\" --region=\\"${AWS_DEFAULT_REGION}\\" --arn=\\"${AWS_CONSOLE_ADMIN_ROLE_ARN}\\" --group system:masters --username admin\\nfi\\n```\\n\\nOutput:\\n\\n```text\\n2021-11-29 18:23:13 [\u2139]  eksctl version 0.75.0\\n2021-11-29 18:23:13 [\u2139]  using region eu-west-1\\n2021-11-29 18:23:14 [\u2139]  adding identity \\"arn:aws:iam::7xxxxxxxxxx7:role/AxxxxxxxxxxxxN\\" to auth ConfigMap\\n```\\n\\nCheck the nodes+pods and max number of nodes which can be scheduled on one node:\\n\\n```bash\\nkubectl get nodes,pods -o wide --all-namespaces\\n```\\n\\nOutput:\\n\\n```text\\nNAME                                                STATUS   ROLES    AGE   VERSION   INTERNAL-IP      EXTERNAL-IP     OS-IMAGE                               KERNEL-VERSION   CONTAINER-RUNTIME\\nnode/ip-192-168-31-11.eu-west-1.compute.internal    Ready    <none>   81s   v1.21.6   192.168.31.11    54.194.69.158   Bottlerocket OS 1.4.1 (aws-k8s-1.21)   5.10.68          containerd://1.5.5+bottlerocket\\nnode/ip-192-168-56-82.eu-west-1.compute.internal    Ready    <none>   86s   v1.21.6   192.168.56.82    3.250.52.238    Bottlerocket OS 1.4.1 (aws-k8s-1.21)   5.10.68          containerd://1.5.5+bottlerocket\\nnode/ip-192-168-60-184.eu-west-1.compute.internal   Ready    <none>   84s   v1.21.6   192.168.60.184   54.75.89.58     Bottlerocket OS 1.4.1 (aws-k8s-1.21)   5.10.68          containerd://1.5.5+bottlerocket\\n\\nNAMESPACE     NAME                                           READY   STATUS    RESTARTS   AGE     IP               NODE                                           NOMINATED NODE   READINESS GATES\\nkube-system   pod/calico-kube-controllers-6c85b56fcb-5g5cq   1/1     Running   0          4m16s   172.16.166.130   ip-192-168-56-82.eu-west-1.compute.internal    <none>           <none>\\nkube-system   pod/calico-node-4whs8                          1/1     Running   0          84s     192.168.60.184   ip-192-168-60-184.eu-west-1.compute.internal   <none>           <none>\\nkube-system   pod/calico-node-nbjsn                          1/1     Running   0          86s     192.168.56.82    ip-192-168-56-82.eu-west-1.compute.internal    <none>           <none>\\nkube-system   pod/calico-node-nnhwz                          1/1     Running   0          81s     192.168.31.11    ip-192-168-31-11.eu-west-1.compute.internal    <none>           <none>\\nkube-system   pod/coredns-7cc879f8db-ct5bp                   1/1     Running   0          21m     172.16.166.131   ip-192-168-56-82.eu-west-1.compute.internal    <none>           <none>\\nkube-system   pod/coredns-7cc879f8db-h4mbs                   1/1     Running   0          21m     172.16.166.129   ip-192-168-56-82.eu-west-1.compute.internal    <none>           <none>\\nkube-system   pod/kube-proxy-9c9wk                           1/1     Running   0          86s     192.168.56.82    ip-192-168-56-82.eu-west-1.compute.internal    <none>           <none>\\nkube-system   pod/kube-proxy-gqbt7                           1/1     Running   0          81s     192.168.31.11    ip-192-168-31-11.eu-west-1.compute.internal    <none>           <none>\\nkube-system   pod/kube-proxy-q7pzh                           1/1     Running   0          84s     192.168.60.184   ip-192-168-60-184.eu-west-1.compute.internal   <none>           <none>\\n```"},{"id":"image-test-eks-post","metadata":{"permalink":"/docosaurus-test/blog/image-test-eks-post","editUrl":"https://github.com/ruzickap/blog-test.ruzicka.dev/tree/main/docosaurus-test/blog/2020/2020-12-01-image-test/README.md","source":"@site/blog/2020/2020-12-01-image-test/README.md","title":"Image Test EKS Post","description":"Keycloak","date":"2020-12-01T00:00:00.000Z","tags":[{"inline":true,"label":"EKS","permalink":"/docosaurus-test/blog/tags/eks"},{"inline":true,"label":"Image","permalink":"/docosaurus-test/blog/tags/image"},{"inline":true,"label":"Pipeline Templates","permalink":"/docosaurus-test/blog/tags/pipeline-templates"}],"readingTime":7.1,"hasTruncateMarker":false,"authors":[{"name":"Petr Ruzicka","title":"Blog owner","url":"https://petr.ruzicka.dev","imageURL":"https://github.com/ruzickap.png","key":"Petr Ruzicka"}],"frontMatter":{"slug":"image-test-eks-post","title":"Image Test EKS Post","authors":"Petr Ruzicka","categories":["EKS","Image","Pipelines"],"tags":["EKS","Image","Pipeline Templates"]},"unlisted":false,"prevItem":{"title":"Test EKS Post","permalink":"/docosaurus-test/blog/test-eks-post"},"nextItem":{"title":"Long Blog Post","permalink":"/docosaurus-test/blog/long-blog-post"}},"content":"## Keycloak\\n\\nInstall `keycloak`\\n[helm chart](https://artifacthub.io/packages/helm/bitnami/keycloak)\\nand modify the\\n[default values](https://github.com/bitnami/charts/blob/master/bitnami/keycloak/values.yaml).\\n\\n```bash\\nhelm repo add --force-update bitnami https://charts.bitnami.com/bitnami\\nhelm upgrade --install --version 5.0.6 --namespace keycloak --create-namespace --values - keycloak bitnami/keycloak << EOF\\nclusterDomain: ${CLUSTER_FQDN}\\nauth:\\n  adminUser: admin\\n  adminPassword: ${MY_PASSWORD}\\n  managementUser: admin\\n  managementPassword: ${MY_PASSWORD}\\nproxyAddressForwarding: true\\n# https://stackoverflow.com/questions/51616770/keycloak-restricting-user-management-to-certain-groups-while-enabling-manage-us\\nextraStartupArgs: \\"-Dkeycloak.profile.feature.admin_fine_grained_authz=enabled\\"\\n# keycloakConfigCli:\\n#   enabled: true\\n#   # Workaround for bug: https://github.com/bitnami/charts/issues/6823\\n#   image:\\n#     repository: adorsys/keycloak-config-cli\\n#     tag: latest-15.0.1\\n#   configuration:\\n#     myrealm.yaml: |\\n#       realm: myrealm\\n#       enabled: true\\n#       displayName: My Realm\\n#       rememberMe: true\\n#       userManagedAccessAllowed: true\\n#       smtpServer:\\n#         from: myrealm-keycloak@${CLUSTER_FQDN}\\n#         fromDisplayName: Keycloak\\n#         host: mailhog.mailhog.svc.cluster.local\\n#         port: 1025\\n#       clients:\\n#       # https://oauth2-proxy.github.io/oauth2-proxy/docs/configuration/oauth_provider/#keycloak-auth-provider\\n#       - clientId: oauth2-proxy-keycloak.${CLUSTER_FQDN}\\n#         name: oauth2-proxy-keycloak.${CLUSTER_FQDN}\\n#         description: \\"OAuth2 Proxy for Keycloak\\"\\n#         secret: ${MY_PASSWORD}\\n#         redirectUris:\\n#         - \\"https://oauth2-proxy-keycloak.${CLUSTER_FQDN}/oauth2/callback\\"\\n#         protocolMappers:\\n#         - name: groupMapper\\n#           protocol: openid-connect\\n#           protocolMapper: oidc-group-membership-mapper\\n#           config:\\n#             userinfo.token.claim: \\"true\\"\\n#             id.token.claim: \\"true\\"\\n#             access.token.claim: \\"true\\"\\n#             claim.name: groups\\n#             full.path: \\"true\\"\\n#       identityProviders:\\n#       # https://ultimatesecurity.pro/post/okta-oidc/\\n#       - alias: keycloak-oidc-okta\\n#         displayName: \\"Okta\\"\\n#         providerId: keycloak-oidc\\n#         trustEmail: true\\n#         config:\\n#           clientId: ${OKTA_CLIENT_ID}\\n#           clientSecret: ${OKTA_CLIENT_SECRET}\\n#           tokenUrl: \\"${OKTA_ISSUER}/oauth2/default/v1/token\\"\\n#           authorizationUrl: \\"${OKTA_ISSUER}/oauth2/default/v1/authorize\\"\\n#           defaultScope: \\"openid profile email\\"\\n#           syncMode: IMPORT\\n#       - alias: dex\\n#         displayName: \\"Dex\\"\\n#         providerId: keycloak-oidc\\n#         trustEmail: true\\n#         config:\\n#           clientId: keycloak.${CLUSTER_FQDN}\\n#           clientSecret: ${MY_PASSWORD}\\n#           tokenUrl: https://dex.${CLUSTER_FQDN}/token\\n#           authorizationUrl: https://dex.${CLUSTER_FQDN}/auth\\n#           syncMode: IMPORT\\n#       - alias: github\\n#         displayName: \\"Github\\"\\n#         providerId: github\\n#         trustEmail: true\\n#         config:\\n#           clientId: ${MY_GITHUB_ORG_OAUTH_KEYCLOAK_CLIENT_ID}\\n#           clientSecret: ${MY_GITHUB_ORG_OAUTH_KEYCLOAK_CLIENT_SECRET}\\n#       users:\\n#       - username: myuser1\\n#         email: myuser1@${CLUSTER_FQDN}\\n#         enabled: true\\n#         firstName: My Firstname 1\\n#         lastName: My Lastname 1\\n#         groups:\\n#           - group-admins\\n#         credentials:\\n#         - type: password\\n#           value: ${MY_PASSWORD}\\n#       - username: myuser2\\n#         email: myuser2@${CLUSTER_FQDN}\\n#         enabled: true\\n#         firstName: My Firstname 2\\n#         lastName: My Lastname 2\\n#         groups:\\n#           - group-admins\\n#         credentials:\\n#         - type: password\\n#           value: ${MY_PASSWORD}\\n#       - username: myuser3\\n#         email: myuser3@${CLUSTER_FQDN}\\n#         enabled: true\\n#         firstName: My Firstname 3\\n#         lastName: My Lastname 3\\n#         groups:\\n#           - group-users\\n#         credentials:\\n#         - type: password\\n#           value: ${MY_PASSWORD}\\n#       - username: myuser4\\n#         email: myuser4@${CLUSTER_FQDN}\\n#         enabled: true\\n#         firstName: My Firstname 4\\n#         lastName: My Lastname 4\\n#         groups:\\n#           - group-users\\n#           - group-test\\n#         credentials:\\n#         - type: password\\n#           value: ${MY_PASSWORD}\\n#       groups:\\n#       - name: group-users\\n#       - name: group-admins\\n#       - name: group-test\\nservice:\\n  type: ClusterIP\\ningress:\\n  enabled: true\\n  hostname: keycloak.${CLUSTER_FQDN}\\n  extraTls:\\n  - hosts:\\n      - keycloak.${CLUSTER_FQDN}\\n    secretName: ingress-cert-${LETSENCRYPT_ENVIRONMENT}\\nnetworkPolicy:\\n  enabled: true\\nmetrics:\\n  enabled: true\\n  serviceMonitor:\\n    enabled: true\\npostgresql:\\n  persistence:\\n    enabled: false\\nEOF\\n```\\n\\n## oauth2-proxy - Keycloak\\n\\nInstall [oauth2-proxy](https://github.com/oauth2-proxy/oauth2-proxy) to secure\\nthe endpoints like (`prometheus.`, `alertmanager.`).\\n\\nInstall `oauth2-proxy`\\n[helm chart](https://artifacthub.io/packages/helm/oauth2-proxy/oauth2-proxy)\\nand modify the\\n[default values](https://github.com/oauth2-proxy/manifests/blob/main/helm/oauth2-proxy/values.yaml).\\n\\n```bash\\nhelm repo add --force-update oauth2-proxy https://oauth2-proxy.github.io/manifests\\nhelm upgrade --install --version 4.2.0 --namespace oauth2-proxy-keycloak --create-namespace --values - oauth2-proxy oauth2-proxy/oauth2-proxy << EOF\\nconfig:\\n  clientID: oauth2-proxy-keycloak.${CLUSTER_FQDN}\\n  clientSecret: \\"${MY_PASSWORD}\\"\\n  cookieSecret: \\"$(openssl rand -base64 32 | head -c 32 | base64)\\"\\n  configFile: |-\\n    email_domains = [ \\"*\\" ]\\n    upstreams = [ \\"file:///dev/null\\" ]\\n    whitelist_domains = \\".${CLUSTER_FQDN}\\"\\n    cookie_domains = \\".${CLUSTER_FQDN}\\"\\n    provider = \\"keycloak\\"\\n    login_url = \\"https://keycloak.${CLUSTER_FQDN}/auth/realms/myrealm/protocol/openid-connect/auth\\"\\n    redeem_url = \\"https://keycloak.${CLUSTER_FQDN}/auth/realms/myrealm/protocol/openid-connect/token\\"\\n    profile_url = \\"https://keycloak.${CLUSTER_FQDN}/auth/realms/myrealm/protocol/openid-connect/userinfo\\"\\n    validate_url = \\"https://keycloak.${CLUSTER_FQDN}/auth/realms/myrealm/protocol/openid-connect/userinfo\\"\\n    scope = \\"openid email profile\\"\\n    ssl_insecure_skip_verify = \\"true\\"\\n    insecure_oidc_skip_issuer_verification = \\"true\\"\\ningress:\\n  enabled: true\\n  hosts:\\n    - oauth2-proxy-keycloak.${CLUSTER_FQDN}\\n  tls:\\n    - secretName: ingress-cert-${LETSENCRYPT_ENVIRONMENT}\\n      hosts:\\n        - oauth2-proxy-keycloak.${CLUSTER_FQDN}\\nmetrics:\\n  servicemonitor:\\n    enabled: true\\nEOF\\n```\\n\\n## Dex\\n\\nInstall `dex`\\n[helm chart](https://artifacthub.io/packages/helm/dex/dex)\\nand modify the\\n[default values](https://github.com/dexidp/helm-charts/blob/master/charts/dex/values.yaml).\\n\\n```bash\\nhelm repo add --force-update dex https://charts.dexidp.io\\nhelm upgrade --install --version 0.6.0 --namespace dex --create-namespace --values - dex dex/dex << EOF\\ningress:\\n  enabled: true\\n  annotations:\\n    nginx.ingress.kubernetes.io/ssl-redirect: \\"false\\"\\n  hosts:\\n    - host: dex.${CLUSTER_FQDN}\\n      paths:\\n        - path: /\\n          pathType: ImplementationSpecific\\n  tls:\\n    - secretName: ingress-cert-${LETSENCRYPT_ENVIRONMENT}\\n      hosts:\\n        - dex.${CLUSTER_FQDN}\\nconfig:\\n  issuer: https://dex.${CLUSTER_FQDN}\\n  storage:\\n    type: kubernetes\\n    config:\\n      inCluster: true\\n  oauth2:\\n    skipApprovalScreen: true\\n  connectors:\\n    - type: github\\n      id: github\\n      name: GitHub\\n      config:\\n        clientID: ${MY_GITHUB_ORG_OAUTH_DEX_CLIENT_ID}\\n        clientSecret: ${MY_GITHUB_ORG_OAUTH_DEX_CLIENT_SECRET}\\n        redirectURI: https://dex.${CLUSTER_FQDN}/callback\\n        orgs:\\n          - name: ${MY_GITHUB_ORG_NAME}\\n    - type: oidc\\n      id: okta\\n      name: Okta\\n      config:\\n        issuer: ${OKTA_ISSUER}\\n        clientID: ${OKTA_CLIENT_ID}\\n        clientSecret: ${OKTA_CLIENT_SECRET}\\n        redirectURI: https://dex.${CLUSTER_FQDN}/callback\\n        scopes:\\n          - openid\\n          - profile\\n          - email\\n        getUserInfo: true\\n  staticClients:\\n    - id: argocd.${CLUSTER_FQDN}\\n      redirectURIs:\\n        - https://argocd.${CLUSTER_FQDN}/auth/callback\\n      name: ArgoCD\\n      secret: ${MY_PASSWORD}\\n    - id: gangway.${CLUSTER_FQDN}\\n      redirectURIs:\\n        - https://gangway.${CLUSTER_FQDN}/callback\\n      name: Gangway\\n      secret: ${MY_PASSWORD}\\n    - id: harbor.${CLUSTER_FQDN}\\n      redirectURIs:\\n        - https://harbor.${CLUSTER_FQDN}/c/oidc/callback\\n      name: Harbor\\n      secret: ${MY_PASSWORD}\\n    - id: kiali.${CLUSTER_FQDN}\\n      redirectURIs:\\n        - https://kiali.${CLUSTER_FQDN}\\n      name: Kiali\\n      secret: ${MY_PASSWORD}\\n    - id: keycloak.${CLUSTER_FQDN}\\n      redirectURIs:\\n        - https://keycloak.${CLUSTER_FQDN}/auth/realms/myrealm/broker/dex/endpoint\\n      name: Keycloak\\n      secret: ${MY_PASSWORD}\\n    - id: oauth2-proxy.${CLUSTER_FQDN}\\n      redirectURIs:\\n        - https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/callback\\n      name: OAuth2 Proxy\\n      secret: ${MY_PASSWORD}\\n    - id: vault.${CLUSTER_FQDN}\\n      redirectURIs:\\n        - https://vault.${CLUSTER_FQDN}/ui/vault/auth/oidc/oidc/callback\\n        - http://localhost:8250/oidc/callback\\n      name: Vault\\n      secret: ${MY_PASSWORD}\\n  enablePasswordDB: false\\nEOF\\n```\\n\\n## oauth2-proxy\\n\\nInstall [oauth2-proxy](https://github.com/oauth2-proxy/oauth2-proxy) to secure\\nthe endpoints like (`prometheus.`, `alertmanager.`).\\n\\nInstall `oauth2-proxy`\\n[helm chart](https://artifacthub.io/packages/helm/oauth2-proxy/oauth2-proxy)\\nand modify the\\n[default values](https://github.com/oauth2-proxy/manifests/blob/main/helm/oauth2-proxy/values.yaml).\\n\\n```bash\\nhelm upgrade --install --version 4.2.0 --namespace oauth2-proxy --create-namespace --values - oauth2-proxy oauth2-proxy/oauth2-proxy << EOF\\nconfig:\\n  clientID: oauth2-proxy.${CLUSTER_FQDN}\\n  clientSecret: \\"${MY_PASSWORD}\\"\\n  cookieSecret: \\"$(openssl rand -base64 32 | head -c 32 | base64)\\"\\n  configFile: |-\\n    email_domains = [ \\"*\\" ]\\n    upstreams = [ \\"file:///dev/null\\" ]\\n    whitelist_domains = \\".${CLUSTER_FQDN}\\"\\n    cookie_domains = \\".${CLUSTER_FQDN}\\"\\n    provider = \\"oidc\\"\\n    oidc_issuer_url = \\"https://dex.${CLUSTER_FQDN}\\"\\n    ssl_insecure_skip_verify = \\"true\\"\\n    insecure_oidc_skip_issuer_verification = \\"true\\"\\ningress:\\n  enabled: true\\n  hosts:\\n    - oauth2-proxy.${CLUSTER_FQDN}\\n  tls:\\n    - secretName: ingress-cert-${LETSENCRYPT_ENVIRONMENT}\\n      hosts:\\n        - oauth2-proxy.${CLUSTER_FQDN}\\nmetrics:\\n  servicemonitor:\\n    enabled: true\\nEOF\\n```\\n\\n## Gangway\\n\\nInstall gangway:\\n\\n```bash\\nhelm repo add --force-update stable https://charts.helm.sh/stable\\nhelm upgrade --install --version 0.4.5 --namespace gangway --create-namespace --values - gangway stable/gangway << EOF\\n# https://github.com/helm/charts/blob/master/stable/gangway/values.yaml\\ntrustedCACert: |\\n$(curl -s \\"${LETSENCRYPT_CERTIFICATE}\\" | sed \\"s/^/  /\\")\\ngangway:\\n  clusterName: ${CLUSTER_FQDN}\\n  authorizeURL: https://dex.${CLUSTER_FQDN}/auth\\n  tokenURL: https://dex.${CLUSTER_FQDN}/token\\n  audience: https://dex.${CLUSTER_FQDN}/userinfo\\n  redirectURL: https://gangway.${CLUSTER_FQDN}/callback\\n  clientID: gangway.${CLUSTER_FQDN}\\n  clientSecret: ${MY_PASSWORD}\\n  apiServerURL: https://kube-oidc-proxy.${CLUSTER_FQDN}\\ningress:\\n  enabled: true\\n  annotations:\\n    nginx.ingress.kubernetes.io/auth-url: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/auth\\n    nginx.ingress.kubernetes.io/auth-signin: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/start?rd=\\\\$scheme://\\\\$host\\\\$request_uri\\n  hosts:\\n    - gangway.${CLUSTER_FQDN}\\n  tls:\\n    - secretName: ingress-cert-${LETSENCRYPT_ENVIRONMENT}\\n      hosts:\\n        - gangway.${CLUSTER_FQDN}\\nEOF\\n```\\n\\n## kube-oidc-proxy\\n\\nThe `kube-oidc-proxy` accepting connections only via HTTPS. It\'s necessary to\\nconfigure ingress to communicate with the backend over HTTPS.\\n\\nInstall kube-oidc-proxy:\\n\\n```bash\\ntest -d \\"tmp/${CLUSTER_FQDN}/kube-oidc-proxy\\" || git clone --quiet https://github.com/jetstack/kube-oidc-proxy.git \\"tmp/${CLUSTER_FQDN}/kube-oidc-proxy\\"\\ngit -C \\"tmp/${CLUSTER_FQDN}/kube-oidc-proxy\\" checkout --quiet v0.3.0\\n\\nhelm upgrade --install --namespace kube-oidc-proxy --create-namespace --values - kube-oidc-proxy \\"tmp/${CLUSTER_FQDN}/kube-oidc-proxy/deploy/charts/kube-oidc-proxy\\" << EOF\\n# https://github.com/jetstack/kube-oidc-proxy/blob/master/deploy/charts/kube-oidc-proxy/values.yaml\\noidc:\\n  clientId: gangway.${CLUSTER_FQDN}\\n  issuerUrl: https://dex.${CLUSTER_FQDN}\\n  usernameClaim: email\\n  caPEM: |\\n$(curl -s \\"${LETSENCRYPT_CERTIFICATE}\\" | sed \\"s/^/    /\\")\\ningress:\\n  annotations:\\n    nginx.ingress.kubernetes.io/backend-protocol: HTTPS\\n  enabled: true\\n  hosts:\\n    - host: kube-oidc-proxy.${CLUSTER_FQDN}\\n      paths:\\n        - /\\n  tls:\\n   - secretName: ingress-cert-${LETSENCRYPT_ENVIRONMENT}\\n     hosts:\\n       - kube-oidc-proxy.${CLUSTER_FQDN}\\nEOF\\n```\\n\\nIf you get the credentials form the [https://gangway.kube1.k8s.mylabs.dev](https://gangway.kube1.k8s.mylabs.dev)\\nyou will have the access to the cluster, but no rights there.\\n\\nAdd access rights to the user:\\n\\n```bash\\nkubectl apply -f - << EOF\\napiVersion: rbac.authorization.k8s.io/v1\\nkind: Role\\nmetadata:\\n  namespace: kube-prometheus-stack\\n  name: secret-reader\\nrules:\\n- apiGroups: [\\"\\"]\\n  resources: [\\"secrets\\"]\\n  verbs: [\\"get\\", \\"watch\\", \\"list\\"]\\n---\\napiVersion: rbac.authorization.k8s.io/v1\\nkind: RoleBinding\\nmetadata:\\n  name: read-secrets\\n  namespace: kube-prometheus-stack\\nsubjects:\\n- kind: User\\n  name: ${MY_EMAIL}\\n  apiGroup: rbac.authorization.k8s.io\\nroleRef:\\n  kind: Role\\n  name: secret-reader\\n  apiGroup: rbac.authorization.k8s.io\\n---\\napiVersion: rbac.authorization.k8s.io/v1\\nkind: ClusterRole\\nmetadata:\\n  name: pods-reader\\nrules:\\n- apiGroups: [\\"\\"]\\n  resources: [\\"pods\\"]\\n  verbs: [\\"get\\", \\"watch\\", \\"list\\"]\\n---\\napiVersion: rbac.authorization.k8s.io/v1\\nkind: ClusterRoleBinding\\nmetadata:\\n  name: read-pods\\nsubjects:\\n- kind: User\\n  name: ${MY_EMAIL}\\n  apiGroup: rbac.authorization.k8s.io\\nroleRef:\\n  kind: ClusterRole\\n  name: pods-reader\\n  apiGroup: rbac.authorization.k8s.io\\nEOF\\n```\\n\\nThe user should be able to read the secrets in `kube-prometheus-stack`\\nnamespace:\\n\\n```shell\\nkubectl describe secrets --insecure-skip-tls-verify -n kube-prometheus-stack \\"ingress-cert-${LETSENCRYPT_ENVIRONMENT}\\" # DevSkim: ignore DS126188\\n```\\n\\nOutput:\\n\\n```text\\nName:         ingress-cert-staging\\nNamespace:    kube-prometheus-stack\\nLabels:       kubed.appscode.com/origin.cluster=kube1.k8s.mylabs.dev\\n              kubed.appscode.com/origin.name=ingress-cert-staging\\n              kubed.appscode.com/origin.namespace=cert-manager\\nAnnotations:  cert-manager.io/alt-names: *.kube1.k8s.mylabs.dev,kube1.k8s.mylabs.dev\\n              cert-manager.io/certificate-name: ingress-cert-staging\\n              cert-manager.io/common-name: *.kube1.k8s.mylabs.dev\\n              cert-manager.io/ip-sans:\\n              cert-manager.io/issuer-group:\\n              cert-manager.io/issuer-kind: ClusterIssuer\\n              cert-manager.io/issuer-name: letsencrypt-staging-dns\\n              cert-manager.io/uri-sans:\\n              kubed.appscode.com/origin:\\n                {\\"namespace\\":\\"cert-manager\\",\\"name\\":\\"ingress-cert-staging\\",\\"uid\\":\\"f1ed062c-23d9-4cf7-ad51-cfafd8a3b788\\",\\"resourceVersion\\":\\"5296\\"}\\n\\nType:  kubernetes.io/tls\\n\\nData\\n====\\ntls.crt:  3586 bytes\\ntls.key:  1679 bytes\\n```\\n\\nBut it\'s not allowed to delete the secrets for the user:\\n\\n```shell\\nkubectl delete secrets --insecure-skip-tls-verify -n kube-prometheus-stack \\"ingress-cert-${LETSENCRYPT_ENVIRONMENT}\\"   # DevSkim: ignore DS126188\\n```\\n\\nOutput:\\n\\n```text\\nError from server (Forbidden): secrets \\"ingress-cert-staging\\" is forbidden: User \\"petr.ruzicka@gmail.com\\" cannot delete resource \\"secrets\\" in API group \\"\\" in the namespace \\"kube-prometheus-stack\\"\\n```\\n\\nThe user can not read secrets outside the `kube-prometheus-stack`:\\n\\n```shell\\nkubectl get secrets --insecure-skip-tls-verify -n kube-system                                                          # DevSkim: ignore DS126188\\n```\\n\\nOutput:\\n\\n```text\\nError from server (Forbidden): secrets is forbidden: User \\"petr.ruzicka@gmail.com\\" cannot list resource \\"secrets\\" in API group \\"\\" in the namespace \\"kube-system\\"\\n```\\n\\nYou can see the pods \\"everywhere\\":\\n\\n```shell\\nkubectl get pods --insecure-skip-tls-verify -n kube-system                                                             # DevSkim: ignore DS126188\\n```\\n\\nOutput:\\n\\n```text\\nNAME                                                         READY   STATUS    RESTARTS   AGE\\naws-for-fluent-bit-5hxlt                                     1/1     Running   0          32m\\naws-for-fluent-bit-dmvzq                                     1/1     Running   0          32m\\naws-node-ggfft                                               1/1     Running   0          32m\\naws-node-lhlvf                                               1/1     Running   0          32m\\ncluster-autoscaler-aws-cluster-autoscaler-7f878bccc8-s279k   1/1     Running   0          25m\\ncoredns-59b69b4849-6v487                                     1/1     Running   0          46m\\ncoredns-59b69b4849-tw2dg                                     1/1     Running   0          46m\\nebs-csi-controller-86785d75db-7brbr                          5/5     Running   0          31m\\nebs-csi-controller-86785d75db-gn4ll                          5/5     Running   0          31m\\nebs-csi-node-6h9zv                                           3/3     Running   0          31m\\nebs-csi-node-r5rj7                                           3/3     Running   0          31m\\nkube-proxy-m6dm8                                             1/1     Running   0          32m\\nkube-proxy-pdmv9                                             1/1     Running   0          32m\\n```\\n\\nBut you can not delete them:\\n\\n```shell\\nkubectl delete pods --insecure-skip-tls-verify -n kube-oidc-proxy --all                                                # DevSkim: ignore DS126188\\n```\\n\\nOutput:\\n\\n```text\\nError from server (Forbidden): pods \\"kube-oidc-proxy-74bf5679fd-jhdmr\\" is forbidden: User \\"petr.ruzicka@gmail.com\\" cannot delete resource \\"pods\\" in API group \\"\\" in the namespace \\"kube-oidc-proxy\\"\\n```"},{"id":"long-blog-post","metadata":{"permalink":"/docosaurus-test/blog/long-blog-post","editUrl":"https://github.com/ruzickap/blog-test.ruzicka.dev/tree/main/docosaurus-test/blog/2019-05-29-long-blog-post.md","source":"@site/blog/2019-05-29-long-blog-post.md","title":"Long Blog Post","description":"This is the summary of a very long blog post,","date":"2019-05-29T00:00:00.000Z","tags":[{"inline":true,"label":"hello","permalink":"/docosaurus-test/blog/tags/hello"},{"inline":true,"label":"docusaurus","permalink":"/docosaurus-test/blog/tags/docusaurus"}],"readingTime":2.05,"hasTruncateMarker":true,"authors":[{"name":"Endilie Yacop Sucipto","title":"Maintainer of Docusaurus","url":"https://github.com/endiliey","imageURL":"https://github.com/endiliey.png","key":"endi"}],"frontMatter":{"slug":"long-blog-post","title":"Long Blog Post","authors":"endi","tags":["hello","docusaurus"]},"unlisted":false,"prevItem":{"title":"Image Test EKS Post","permalink":"/docosaurus-test/blog/image-test-eks-post"},"nextItem":{"title":"First Blog Post","permalink":"/docosaurus-test/blog/first-blog-post"}},"content":"This is the summary of a very long blog post,\\n\\nUse a `\x3c!--` `truncate` `--\x3e` comment to limit blog post size in the list view.\\n\\n\x3c!--truncate--\x3e\\n\\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet\\n\\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet\\n\\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet\\n\\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet\\n\\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet\\n\\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet\\n\\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet\\n\\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet\\n\\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet\\n\\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet\\n\\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet\\n\\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet\\n\\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet\\n\\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet\\n\\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet\\n\\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet"},{"id":"first-blog-post","metadata":{"permalink":"/docosaurus-test/blog/first-blog-post","editUrl":"https://github.com/ruzickap/blog-test.ruzicka.dev/tree/main/docosaurus-test/blog/2019-05-28-first-blog-post.md","source":"@site/blog/2019-05-28-first-blog-post.md","title":"First Blog Post","description":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet","date":"2019-05-28T00:00:00.000Z","tags":[{"inline":true,"label":"hola","permalink":"/docosaurus-test/blog/tags/hola"},{"inline":true,"label":"docusaurus","permalink":"/docosaurus-test/blog/tags/docusaurus"}],"readingTime":0.12,"hasTruncateMarker":false,"authors":[{"name":"Gao Wei","title":"Docusaurus Core Team","url":"https://github.com/wgao19","image_url":"https://github.com/wgao19.png","imageURL":"https://github.com/wgao19.png"}],"frontMatter":{"slug":"first-blog-post","title":"First Blog Post","authors":{"name":"Gao Wei","title":"Docusaurus Core Team","url":"https://github.com/wgao19","image_url":"https://github.com/wgao19.png","imageURL":"https://github.com/wgao19.png"},"tags":["hola","docusaurus"]},"unlisted":false,"prevItem":{"title":"Long Blog Post","permalink":"/docosaurus-test/blog/long-blog-post"}},"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet"}]}}')}}]);