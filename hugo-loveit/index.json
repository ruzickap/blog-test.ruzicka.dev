[{"categories":null,"content":"My Projects List of my GitHub projects: https://github.com/ruzickap/ ","date":"2022-10-03","objectID":"/hugo-loveit/projects/:0:0","tags":null,"title":"My Projects","uri":"/hugo-loveit/projects/"},{"categories":null,"content":"GitHub Actions: My Broken Link Checker ✔ Description: A GitHub Action for checking broken links CI/CD status: Issue tracking: Repository: ","date":"2022-10-03","objectID":"/hugo-loveit/projects/:1:0","tags":null,"title":"My Projects","uri":"/hugo-loveit/projects/"},{"categories":null,"content":"ruzickap.github.io Description: blog.ruzicka.dev - personal blog URL: https://ruzickap.github.io/ CI/CD status: Issue tracking: Repository: ","date":"2022-10-03","objectID":"/hugo-loveit/projects/:2:0","tags":null,"title":"My Projects","uri":"/hugo-loveit/projects/"},{"categories":["Test1","Test2"],"content":"Test123 description","date":"2021-10-03","objectID":"/hugo-loveit/2021-10-03-first_post/","tags":["first","first_post"],"title":"First_post","uri":"/hugo-loveit/2021-10-03-first_post/"},{"categories":["Test1","Test2"],"content":"Test 123 Resource: - \"arn:aws:secretsmanager:*:*:secret:*\" ","date":"2021-10-03","objectID":"/hugo-loveit/2021-10-03-first_post/:0:0","tags":["first","first_post"],"title":"First_post","uri":"/hugo-loveit/2021-10-03-first_post/"},{"categories":["EKS","Pipelines"],"content":"Amazon EKS Bottlerocket and Fargate Amazon EKS Before starting with the main content, it’s necessary to provision the Amazon EKS in AWS. ","date":"2020-12-10","objectID":"/hugo-loveit/2020-12-10-eks-test/:0:0","tags":["EKS","Pipeline Templates"],"title":"Test EKS Post","uri":"/hugo-loveit/2020-12-10-eks-test/"},{"categories":["EKS","Pipelines"],"content":"Requirements If you would like to follow this documents and it’s task you will need to set up few environment variables. The LETSENCRYPT_ENVIRONMENT variable should be one of: staging - Let’s Encrypt will create testing certificate (not valid) production - Let’s Encrypt will create valid certificate (use with care) BASE_DOMAIN contains DNS records for all your Kubernetes clusters. The cluster names will look like CLUSTER_NAME.BASE_DOMAIN (kube1.k8s.mylabs.dev). # Hostname / FQDN definitions export BASE_DOMAIN=${BASE_DOMAIN:-k8s.mylabs.dev} export CLUSTER_NAME=${CLUSTER_NAME:-kube1} export CLUSTER_FQDN=\"${CLUSTER_NAME}.${BASE_DOMAIN}\" export KUBECONFIG=${PWD}/kubeconfig-${CLUSTER_NAME}.conf # * \"production\" - valid certificates signed by Lets Encrypt \"\" # * \"staging\" - not trusted certs signed by Lets Encrypt \"Fake LE Intermediate X1\" export LETSENCRYPT_ENVIRONMENT=\"staging\" export LETSENCRYPT_CERTIFICATE=\"https://letsencrypt.org/certs/staging/letsencrypt-stg-root-x1.pem\" # export LETSENCRYPT_ENVIRONMENT=\"production\" # export LETSENCRYPT_CERTIFICATE=\"https://letsencrypt.org/certs/lets-encrypt-r3.pem\" export MY_EMAIL=\"petr.ruzicka@gmail.com\" # GitHub Organization + Team where are the users who will have the admin access # to K8s resources (Grafana). Only users in GitHub organization # (MY_GITHUB_ORG_NAME) will be able to access the apps via ingress. export MY_GITHUB_ORG_NAME=\"ruzickap-org\" export MY_GITHUB_USERNAME=\"ruzickap\" # AWS Region export AWS_DEFAULT_REGION=\"eu-west-1\" export SLACK_CHANNEL=\"mylabs\" # Tags used to tag the AWS resources export TAGS=\"Owner=${MY_EMAIL} Environment=Dev Group=Cloud_Native Squad=Cloud_Container_Platform compliance:na:defender=bottlerocket\" echo -e \"${MY_EMAIL} | ${LETSENCRYPT_ENVIRONMENT} | ${CLUSTER_NAME} | ${BASE_DOMAIN} | ${CLUSTER_FQDN}\\n${TAGS}\" Prepare GitHub OAuth “access” credentials ans AWS “access” variables. You will need to configure AWS CLI: Configuring the AWS CLI # Common password export MY_PASSWORD=\"xxxx\" # AWS Credentials export AWS_ACCESS_KEY_ID=\"\" export AWS_SECRET_ACCESS_KEY=\"\" export AWS_CONSOLE_ADMIN_ROLE_ARN=\"arn:aws:iam::7xxxxxxxxxx7:role/xxxxxxxxxxxxxN\" # GitHub Organization OAuth Apps credentials export MY_GITHUB_ORG_OAUTH_DEX_CLIENT_ID=\"3xxxxxxxxxxxxxxxxxx3\" export MY_GITHUB_ORG_OAUTH_DEX_CLIENT_SECRET=\"7xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx8\" export MY_GITHUB_ORG_OAUTH_KEYCLOAK_CLIENT_ID=\"4xxxxxxxxxxxxxxxxxx4\" export MY_GITHUB_ORG_OAUTH_KEYCLOAK_CLIENT_SECRET=\"7xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxa\" # Sysdig credentials export SYSDIG_AGENT_ACCESSKEY=\"xxx\" # Aqua credentials export AQUA_REGISTRY_USERNAME=\"xxx\" export AQUA_REGISTRY_PASSWORD=\"xxx\" export AQUA_ENFORCER_TOKEN=\"xxx\" # Splunk credentials export SPLUNK_HOST=\"xxx\" export SPLUNK_TOKEN=\"xxx\" export SPLUNK_INDEX_NAME=\"xxx\" # Slack incoming webhook export SLACK_INCOMING_WEBHOOK_URL=\"https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK\" export SLACK_BOT_API_TOKEN=\"xxxx-xxxxxxxxxxxxx-xxxxxxxxxxxxx-xxxxxxxxxxxxxxxxxxxxxxxP\" # Okta configuration export OKTA_ISSUER=\"https://exxxxxxx-xxxxx-xx.okta.com\" export OKTA_CLIENT_ID=\"0xxxxxxxxxxxxxxxxxx7\" export OKTA_CLIENT_SECRET=\"1xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxH\" Verify if all the necessary variables were set: case \"${CLUSTER_NAME}\" in kube1) MY_GITHUB_ORG_OAUTH_DEX_CLIENT_ID=${MY_GITHUB_ORG_OAUTH_DEX_CLIENT_ID:-${MY_GITHUB_ORG_OAUTH_DEX_CLIENT_ID_KUBE1}} MY_GITHUB_ORG_OAUTH_DEX_CLIENT_SECRET=${MY_GITHUB_ORG_OAUTH_DEX_CLIENT_SECRET:-${MY_GITHUB_ORG_OAUTH_DEX_CLIENT_SECRET_KUBE1}} MY_GITHUB_ORG_OAUTH_KEYCLOAK_CLIENT_ID=${MY_GITHUB_ORG_OAUTH_KEYCLOAK_CLIENT_ID:-${MY_GITHUB_ORG_OAUTH_KEYCLOAK_CLIENT_ID_KUBE1}} MY_GITHUB_ORG_OAUTH_KEYCLOAK_CLIENT_SECRET=${MY_GITHUB_ORG_OAUTH_KEYCLOAK_CLIENT_SECRET:-${MY_GITHUB_ORG_OAUTH_KEYCLOAK_CLIENT_SECRET_KUBE1}} ;; kube2) MY_GITHUB_ORG_OAUTH_DEX_CLIENT_ID=${MY_GITHUB_ORG_OAUTH_DEX_CLIENT_ID:-${MY_GITHUB_ORG_OAUTH_DEX_CLIENT_ID_KUBE2}} MY_GITHUB_ORG_OAUTH_DEX_CLIENT_SECRET=${MY_GITHUB_ORG_OAUTH_DEX_CLIENT_SECRET:-${MY_GITHUB_ORG_OAUT","date":"2020-12-10","objectID":"/hugo-loveit/2020-12-10-eks-test/:1:0","tags":["EKS","Pipeline Templates"],"title":"Test EKS Post","uri":"/hugo-loveit/2020-12-10-eks-test/"},{"categories":["EKS","Pipelines"],"content":"Prepare the local working environment ::: tip You can skip these steps if you have all the required software already installed. ::: Install necessary software: if command -v apt-get \u0026\u003e /dev/null; then apt update -qq DEBIAN_FRONTEND=noninteractive apt-get install -y -qq apache2-utils ansible dnsutils git gnupg2 jq sudo unzip \u003e /dev/null fi Install AWS CLI binary: if ! command -v aws \u0026\u003e /dev/null; then curl -sL \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"/tmp/awscliv2.zip\" unzip -q -o /tmp/awscliv2.zip -d /tmp/ sudo /tmp/aws/install fi Install kubectl binary: if ! command -v kubectl \u0026\u003e /dev/null; then # https://github.com/kubernetes/kubectl/releases sudo curl -s -Lo /usr/local/bin/kubectl \"https://storage.googleapis.com/kubernetes-release/release/v1.21.1/bin/$(uname | sed \"s/./\\L\u0026/g\")/amd64/kubectl\" sudo chmod a+x /usr/local/bin/kubectl fi Install Helm: if ! command -v helm \u0026\u003e /dev/null; then # https://github.com/helm/helm/releases curl -s https://raw.githubusercontent.com/helm/helm/master/scripts/get | bash -s -- --version v3.6.0 fi Install eksctl: if ! command -v eksctl \u0026\u003e /dev/null; then # https://github.com/weaveworks/eksctl/releases curl -s -L \"https://github.com/weaveworks/eksctl/releases/download/0.60.0/eksctl_$(uname)_amd64.tar.gz\" | sudo tar xz -C /usr/local/bin/ fi Install AWS IAM Authenticator for Kubernetes: if ! command -v aws-iam-authenticator \u0026\u003e /dev/null; then # https://docs.aws.amazon.com/eks/latest/userguide/install-aws-iam-authenticator.html sudo curl -s -Lo /usr/local/bin/aws-iam-authenticator \"https://amazon-eks.s3.us-west-2.amazonaws.com/1.19.6/2021-01-05/bin/$(uname | sed \"s/./\\L\u0026/g\")/amd64/aws-iam-authenticator\" sudo chmod a+x /usr/local/bin/aws-iam-authenticator fi Install vault: if ! command -v vault \u0026\u003e /dev/null; then curl -s -L \"https://releases.hashicorp.com/vault/1.7.2/vault_1.7.2_$(uname | sed \"s/./\\L\u0026/g\")_amd64.zip\" -o /tmp/vault.zip sudo unzip -q /tmp/vault.zip -d /usr/local/bin/ rm /tmp/vault.zip fi Install velero: if ! command -v velero \u0026\u003e /dev/null; then curl -s -L \"https://github.com/vmware-tanzu/velero/releases/download/v1.6.0/velero-v1.6.0-$(uname | sed \"s/./\\L\u0026/g\")-amd64.tar.gz\" -o /tmp/velero.tar.gz sudo tar xzf /tmp/velero.tar.gz -C /usr/local/bin/ --strip-components 1 \"velero-v1.6.0-$(uname | sed \"s/./\\L\u0026/g\")-amd64/velero\" fi Install flux: if ! command -v flux \u0026\u003e /dev/null; then curl -s https://fluxcd.io/install.sh | sudo bash fi Install calicoctl: if ! command -v calicoctl \u0026\u003e /dev/null; then sudo curl -s -Lo /usr/local/bin/calicoctl https://github.com/projectcalico/calicoctl/releases/download/v3.20.0/calicoctl sudo chmod a+x /usr/local/bin/calicoctl fi Install SOPS: Secrets OPerationS: if ! command -v sops \u0026\u003e /dev/null; then sudo curl -s -Lo /usr/local/bin/sops \"https://github.com/mozilla/sops/releases/download/v3.7.1/sops-v3.7.1.$(uname | sed \"s/./\\L\u0026/g\")\" sudo chmod a+x /usr/local/bin/sops fi Install kustomize: if ! command -v kustomize \u0026\u003e /dev/null; then curl -s \"https://raw.githubusercontent.com/kubernetes-sigs/kustomize/master/hack/install_kustomize.sh\" | sudo bash -s 4.1.2 /usr/local/bin/ fi Install hey: if ! command -v hey \u0026\u003e /dev/null; then sudo curl -s -Lo /usr/local/bin/hey \"https://hey-release.s3.us-east-2.amazonaws.com/hey_$(uname | sed \"s/./\\L\u0026/g\")_amd64\" sudo chmod a+x /usr/local/bin/hey fi ","date":"2020-12-10","objectID":"/hugo-loveit/2020-12-10-eks-test/:2:0","tags":["EKS","Pipeline Templates"],"title":"Test EKS Post","uri":"/hugo-loveit/2020-12-10-eks-test/"},{"categories":["EKS","Pipelines"],"content":"Configure AWS Route 53 Domain delegation Create DNS zone (BASE_DOMAIN): aws route53 create-hosted-zone --output json \\ --name \"${BASE_DOMAIN}\" \\ --caller-reference \"$(date)\" \\ --hosted-zone-config=\"{\\\"Comment\\\": \\\"Created by ${MY_EMAIL}\\\", \\\"PrivateZone\\\": false}\" | jq Use your domain registrar to change the nameservers for your zone (for example “mylabs.dev”) to use the Amazon Route 53 nameservers. Here is the way how you can find out the the Route 53 nameservers: NEW_ZONE_ID=$(aws route53 list-hosted-zones --query \"HostedZones[?Name==\\`${BASE_DOMAIN}.\\`].Id\" --output text) NEW_ZONE_NS=$(aws route53 get-hosted-zone --output json --id \"${NEW_ZONE_ID}\" --query \"DelegationSet.NameServers\") NEW_ZONE_NS1=$(echo \"${NEW_ZONE_NS}\" | jq -r \".[0]\") NEW_ZONE_NS2=$(echo \"${NEW_ZONE_NS}\" | jq -r \".[1]\") Create the NS record in k8s.mylabs.dev (BASE_DOMAIN) for proper zone delegation. This step depends on your domain registrar - I’m using CloudFlare and using Ansible to automate it: ansible -m cloudflare_dns -c local -i \"localhost,\" localhost -a \"zone=mylabs.dev record=${BASE_DOMAIN} type=NS value=${NEW_ZONE_NS1} solo=true proxied=no account_email=${CLOUDFLARE_EMAIL} account_api_token=${CLOUDFLARE_API_KEY}\" ansible -m cloudflare_dns -c local -i \"localhost,\" localhost -a \"zone=mylabs.dev record=${BASE_DOMAIN} type=NS value=${NEW_ZONE_NS2} solo=false proxied=no account_email=${CLOUDFLARE_EMAIL} account_api_token=${CLOUDFLARE_API_KEY}\" Output: localhost | CHANGED =\u003e { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python\" }, \"changed\": true, \"result\": { \"record\": { \"content\": \"ns-885.awsdns-46.net\", \"created_on\": \"2020-11-13T06:25:32.18642Z\", \"id\": \"dxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxb\", \"locked\": false, \"meta\": { \"auto_added\": false, \"managed_by_apps\": false, \"managed_by_argo_tunnel\": false, \"source\": \"primary\" }, \"modified_on\": \"2020-11-13T06:25:32.18642Z\", \"name\": \"k8s.mylabs.dev\", \"proxiable\": false, \"proxied\": false, \"ttl\": 1, \"type\": \"NS\", \"zone_id\": \"2xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxe\", \"zone_name\": \"mylabs.dev\" } } } localhost | CHANGED =\u003e { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python\" }, \"changed\": true, \"result\": { \"record\": { \"content\": \"ns-1692.awsdns-19.co.uk\", \"created_on\": \"2020-11-13T06:25:37.605605Z\", \"id\": \"9xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxb\", \"locked\": false, \"meta\": { \"auto_added\": false, \"managed_by_apps\": false, \"managed_by_argo_tunnel\": false, \"source\": \"primary\" }, \"modified_on\": \"2020-11-13T06:25:37.605605Z\", \"name\": \"k8s.mylabs.dev\", \"proxiable\": false, \"proxied\": false, \"ttl\": 1, \"type\": \"NS\", \"zone_id\": \"2xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxe\", \"zone_name\": \"mylabs.dev\" } } } ","date":"2020-12-10","objectID":"/hugo-loveit/2020-12-10-eks-test/:3:0","tags":["EKS","Pipeline Templates"],"title":"Test EKS Post","uri":"/hugo-loveit/2020-12-10-eks-test/"},{"categories":["EKS","Pipelines"],"content":"Add new domain to Route 53, Policies, S3, EBS Details with examples are described on these links: https://aws.amazon.com/blogs/opensource/introducing-fine-grained-iam-roles-service-accounts/ https://cert-manager.io/docs/configuration/acme/dns01/route53/ https://github.com/kubernetes-sigs/external-dns/blob/master/docs/tutorials/aws.md Create CloudFormation template containing policies for Route53, S3 access (Harbor, Velero) and Domain. Put new domain CLUSTER_FQDN to the Route 53 and configure the DNS delegation from the BASE_DOMAIN. mkdir -vp \"tmp/${CLUSTER_FQDN}\" cat \u003e \"tmp/${CLUSTER_FQDN}/aws-route53-iam-s3-kms-asm.yml\" \u003c\u003c \\EOF Description: \"Template to generate the necessary IAM Policies for access to Route53 and S3\" Parameters: ClusterFQDN: Description: \"Cluster domain where all necessary app subdomains will live (subdomain of BaseDomain). Ex: kube1.k8s.mylabs.dev\" Type: String ClusterName: Description: \"Cluster Name Ex: kube1\" Type: String BaseDomain: Description: \"Base domain where cluster domains + their subdomains will live. Ex: k8s.mylabs.dev\" Type: String Resources: # This AWS control checks whether the status of the AWS Systems Manager association compliance is COMPLIANT or NON_COMPLIANT after the association is executed on an instance. ConfigRule: Type: \"AWS::Config::ConfigRule\" Properties: ConfigRuleName: !Sub \"${ClusterName}-ec2-managedinstance-association-compliance-status-check\" Scope: ComplianceResourceTypes: - \"AWS::SSM::AssociationCompliance\" Description: \"A Config rule that checks whether the compliance status of the Amazon EC2 Systems Manager association compliance is COMPLIANT or NON_COMPLIANT after the association execution on the instance. The rule is compliant if the field status is COMPLIANT.\" Source: Owner: \"AWS\" SourceIdentifier: \"EC2_MANAGEDINSTANCE_ASSOCIATION_COMPLIANCE_STATUS_CHECK\" CloudWatchPolicy: Type: AWS::IAM::ManagedPolicy Properties: ManagedPolicyName: !Sub \"${ClusterFQDN}-CloudWatch\" Description: !Sub \"Policy required by Fargate to log to CloudWatch for ${ClusterFQDN}\" PolicyDocument: Version: \"2012-10-17\" Statement: - Effect: Allow Action: - logs:CreateLogStream - logs:CreateLogGroup - logs:DescribeLogStreams - logs:PutLogEvents Resource: \"*\" HostedZone: Type: AWS::Route53::HostedZone Properties: Name: !Ref ClusterFQDN KMSAlias: Type: AWS::KMS::Alias Properties: AliasName: !Sub \"alias/eks-${ClusterName}\" TargetKeyId: !Ref KMSKey KMSKey: Type: AWS::KMS::Key Properties: Description: !Sub \"KMS key for secrets related to ${ClusterFQDN}\" EnableKeyRotation: true PendingWindowInDays: 7 KeyPolicy: Version: \"2012-10-17\" Id: !Sub \"eks-key-policy-${ClusterName}\" Statement: - Sid: Enable IAM User Permissions Effect: Allow Principal: AWS: !Sub \"arn:aws:iam::${AWS::AccountId}:root\" Action: kms:* Resource: \"*\" - Sid: Allow use of the key Effect: Allow Principal: AWS: !Sub \"arn:aws:iam::${AWS::AccountId}:role/aws-service-role/autoscaling.amazonaws.com/AWSServiceRoleForAutoScaling\" Action: - kms:Encrypt - kms:Decrypt - kms:ReEncrypt* - kms:GenerateDataKey* - kms:DescribeKey Resource: \"*\" - Sid: Allow attachment of persistent resources Effect: Allow Principal: AWS: !Sub \"arn:aws:iam::${AWS::AccountId}:role/aws-service-role/autoscaling.amazonaws.com/AWSServiceRoleForAutoScaling\" Action: - kms:CreateGrant Resource: \"*\" Condition: Bool: kms:GrantIsForAWSResource: true EKSViewNodesAndWorkloadsPolicy: Type: AWS::IAM::ManagedPolicy Properties: ManagedPolicyName: !Sub \"${ClusterFQDN}-EKSViewNodesAndWorkloads\" Description: !Sub \"Policy used to view workloads running in an EKS cluster created using CAPA\" PolicyDocument: Version: \"2012-10-17\" Statement: - Effect: Allow Action: - eks:DescribeNodegroup - eks:ListNodegroups - eks:DescribeCluster - eks:ListClusters - eks:AccessKubernetesApi - ssm:GetParameter - eks:ListUpdates - eks:ListFargateProfiles Resource: \"*\" RecordSet: Type: AWS::Route53::RecordSet Properties: HostedZoneName: !Sub \"${BaseDomain}.\" Name: !Ref ClusterFQDN Type: NS TTL: 60 ResourceRecords: !GetAt","date":"2020-12-10","objectID":"/hugo-loveit/2020-12-10-eks-test/:4:0","tags":["EKS","Pipeline Templates"],"title":"Test EKS Post","uri":"/hugo-loveit/2020-12-10-eks-test/"},{"categories":["EKS","Pipelines"],"content":"Create Amazon EKS EKS Create Amazon EKS in AWS by using eksctl. It’s a tool from Weaveworks based on official AWS CloudFormation templates which will be used to launch and configure our EKS cluster and nodes. eksctl Generate SSH key if not exists: test -f ~/.ssh/id_rsa.pub || (install -m 0700 -d ~/.ssh \u0026\u0026 ssh-keygen -b 2048 -t rsa -f ~/.ssh/id_rsa -q -N \"\") Create the Amazon EKS cluster with Calico using eksctl: cat \u003e \"tmp/${CLUSTER_FQDN}/eksctl.yaml\" \u003c\u003c EOF apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: ${CLUSTER_NAME} region: ${AWS_DEFAULT_REGION} version: \"1.21\" tags: \u0026tags $(echo \"${TAGS}\" | sed \"s/ /\\\\n /g; s/^/ /g; s/=/: /g\") availabilityZones: - ${AWS_DEFAULT_REGION}a - ${AWS_DEFAULT_REGION}b iam: withOIDC: true serviceAccounts: - metadata: name: aws-load-balancer-controller namespace: kube-system wellKnownPolicies: awsLoadBalancerController: true - metadata: name: cert-manager namespace: cert-manager wellKnownPolicies: certManager: true - metadata: name: cluster-autoscaler namespace: kube-system wellKnownPolicies: autoScaler: true - metadata: name: external-dns namespace: external-dns wellKnownPolicies: externalDNS: true - metadata: name: ebs-csi-controller-sa namespace: kube-system wellKnownPolicies: ebsCSIController: true - metadata: name: harbor namespace: harbor attachPolicyARNs: - ${S3_POLICY_ARN} - metadata: name: velero namespace: velero attachPolicyARNs: - ${S3_POLICY_ARN} - metadata: name: s3-test namespace: s3-test attachPolicyARNs: - ${S3_POLICY_ARN} - metadata: name: grafana namespace: kube-prometheus-stack attachPolicyARNs: - arn:aws:iam::aws:policy/AmazonPrometheusQueryAccess - arn:aws:iam::aws:policy/CloudWatchReadOnlyAccess attachPolicy: Version: 2012-10-17 Statement: - Sid: AllowReadingTagsInstancesRegionsFromEC2 Effect: Allow Action: - ec2:DescribeTags - ec2:DescribeInstances - ec2:DescribeRegions Resource: \"*\" - Sid: AllowReadingResourcesForTags Effect: Allow Action: tag:GetResources Resource: \"*\" - metadata: name: kube-prometheus-stack-prometheus namespace: kube-prometheus-stack attachPolicyARNs: - arn:aws:iam::aws:policy/AmazonPrometheusQueryAccess - arn:aws:iam::aws:policy/AmazonPrometheusRemoteWriteAccess - metadata: name: efs-csi-controller-sa namespace: kube-system wellKnownPolicies: efsCSIController: true - metadata: name: vault namespace: vault attachPolicy: Version: 2012-10-17 Statement: - Sid: VaultKMSUnseal Effect: Allow Action: - kms:Encrypt - kms:Decrypt - kms:DescribeKey Resource: - \"${KMS_KEY_ARN}\" - metadata: name: kuard namespace: kuard attachPolicy: Version: 2012-10-17 Statement: - Sid: AllowSecretManagerAccess Effect: Allow Action: - secretsmanager:GetSecretValue - secretsmanager:DescribeSecret Resource: - \"arn:aws:secretsmanager:*:*:secret:*\" - Sid: AllowKMSAccess Effect: Allow Action: - kms:Decrypt Resource: - \"${KMS_KEY_ARN}\" vpc: nat: gateway: Disable managedNodeGroups: - name: managed-ng-1 amiFamily: Bottlerocket instanceType: t3.xlarge instancePrefix: ruzickap desiredCapacity: 3 minSize: 2 maxSize: 5 volumeSize: 30 labels: role: worker tags: *tags iam: withAddonPolicies: autoScaler: true cloudWatch: true ebs: true efs: true maxPodsPerNode: 1000 volumeEncrypted: true volumeKmsKeyID: ${KMS_KEY_ID} fargateProfiles: - name: fp-fgtest selectors: - namespace: fgtest tags: *tags secretsEncryption: keyARN: ${KMS_KEY_ARN} cloudWatch: clusterLogging: enableTypes: - authenticator EOF if ! eksctl get clusters --name=\"${CLUSTER_NAME}\" \u0026\u003e /dev/null; then eksctl create cluster --config-file \"tmp/${CLUSTER_FQDN}/eksctl.yaml\" --kubeconfig \"${KUBECONFIG}\" --without-nodegroup kubectl delete daemonset -n kube-system aws-node kubectl apply -f https://docs.projectcalico.org/archive/v3.20/manifests/calico-vxlan.yaml eksctl create nodegroup --config-file \"tmp/${CLUSTER_FQDN}/eksctl.yaml\" fi Output: 2021-11-29 17:52:50 [ℹ] eksctl version 0.75.0 2021-11-29 17:52:50 [ℹ] using region eu-west-1 2021-11-29 17:52:50 [ℹ] subnets for eu-west-1a - public:192.168.0.0/19 private:192.168.64.0","date":"2020-12-10","objectID":"/hugo-loveit/2020-12-10-eks-test/:5:0","tags":["EKS","Pipeline Templates"],"title":"Test EKS Post","uri":"/hugo-loveit/2020-12-10-eks-test/"},{"categories":["EKS","Image","Pipelines"],"content":"Authentication ","date":"0001-01-01","objectID":"/hugo-loveit/readme/:0:0","tags":["EKS","Image","Pipeline Templates"],"title":"Image Test EKS Post","uri":"/hugo-loveit/readme/"},{"categories":["EKS","Image","Pipelines"],"content":"Keycloak Install keycloak helm chart and modify the default values. helm repo add --force-update bitnami https://charts.bitnami.com/bitnami helm upgrade --install --version 5.0.6 --namespace keycloak --create-namespace --values - keycloak bitnami/keycloak \u003c\u003c EOF clusterDomain: ${CLUSTER_FQDN} auth: adminUser: admin adminPassword: ${MY_PASSWORD} managementUser: admin managementPassword: ${MY_PASSWORD} proxyAddressForwarding: true # https://stackoverflow.com/questions/51616770/keycloak-restricting-user-management-to-certain-groups-while-enabling-manage-us extraStartupArgs: \"-Dkeycloak.profile.feature.admin_fine_grained_authz=enabled\" # keycloakConfigCli: # enabled: true # # Workaround for bug: https://github.com/bitnami/charts/issues/6823 # image: # repository: adorsys/keycloak-config-cli # tag: latest-15.0.1 # configuration: # myrealm.yaml: | # realm: myrealm # enabled: true # displayName: My Realm # rememberMe: true # userManagedAccessAllowed: true # smtpServer: # from: myrealm-keycloak@${CLUSTER_FQDN} # fromDisplayName: Keycloak # host: mailhog.mailhog.svc.cluster.local # port: 1025 # clients: # # https://oauth2-proxy.github.io/oauth2-proxy/docs/configuration/oauth_provider/#keycloak-auth-provider # - clientId: oauth2-proxy-keycloak.${CLUSTER_FQDN} # name: oauth2-proxy-keycloak.${CLUSTER_FQDN} # description: \"OAuth2 Proxy for Keycloak\" # secret: ${MY_PASSWORD} # redirectUris: # - \"https://oauth2-proxy-keycloak.${CLUSTER_FQDN}/oauth2/callback\" # protocolMappers: # - name: groupMapper # protocol: openid-connect # protocolMapper: oidc-group-membership-mapper # config: # userinfo.token.claim: \"true\" # id.token.claim: \"true\" # access.token.claim: \"true\" # claim.name: groups # full.path: \"true\" # identityProviders: # # https://ultimatesecurity.pro/post/okta-oidc/ # - alias: keycloak-oidc-okta # displayName: \"Okta\" # providerId: keycloak-oidc # trustEmail: true # config: # clientId: ${OKTA_CLIENT_ID} # clientSecret: ${OKTA_CLIENT_SECRET} # tokenUrl: \"${OKTA_ISSUER}/oauth2/default/v1/token\" # authorizationUrl: \"${OKTA_ISSUER}/oauth2/default/v1/authorize\" # defaultScope: \"openid profile email\" # syncMode: IMPORT # - alias: dex # displayName: \"Dex\" # providerId: keycloak-oidc # trustEmail: true # config: # clientId: keycloak.${CLUSTER_FQDN} # clientSecret: ${MY_PASSWORD} # tokenUrl: https://dex.${CLUSTER_FQDN}/token # authorizationUrl: https://dex.${CLUSTER_FQDN}/auth # syncMode: IMPORT # - alias: github # displayName: \"Github\" # providerId: github # trustEmail: true # config: # clientId: ${MY_GITHUB_ORG_OAUTH_KEYCLOAK_CLIENT_ID} # clientSecret: ${MY_GITHUB_ORG_OAUTH_KEYCLOAK_CLIENT_SECRET} # users: # - username: myuser1 # email: myuser1@${CLUSTER_FQDN} # enabled: true # firstName: My Firstname 1 # lastName: My Lastname 1 # groups: # - group-admins # credentials: # - type: password # value: ${MY_PASSWORD} # - username: myuser2 # email: myuser2@${CLUSTER_FQDN} # enabled: true # firstName: My Firstname 2 # lastName: My Lastname 2 # groups: # - group-admins # credentials: # - type: password # value: ${MY_PASSWORD} # - username: myuser3 # email: myuser3@${CLUSTER_FQDN} # enabled: true # firstName: My Firstname 3 # lastName: My Lastname 3 # groups: # - group-users # credentials: # - type: password # value: ${MY_PASSWORD} # - username: myuser4 # email: myuser4@${CLUSTER_FQDN} # enabled: true # firstName: My Firstname 4 # lastName: My Lastname 4 # groups: # - group-users # - group-test # credentials: # - type: password # value: ${MY_PASSWORD} # groups: # - name: group-users # - name: group-admins # - name: group-test service: type: ClusterIP ingress: enabled: true hostname: keycloak.${CLUSTER_FQDN} extraTls: - hosts: - keycloak.${CLUSTER_FQDN} secretName: ingress-cert-${LETSENCRYPT_ENVIRONMENT} networkPolicy: enabled: true metrics: enabled: true serviceMonitor: enabled: true postgresql: persistence: enabled: false EOF ","date":"0001-01-01","objectID":"/hugo-loveit/readme/:1:0","tags":["EKS","Image","Pipeline Templates"],"title":"Image Test EKS Post","uri":"/hugo-loveit/readme/"},{"categories":["EKS","Image","Pipelines"],"content":"oauth2-proxy - Keycloak Install oauth2-proxy to secure the endpoints like (prometheus., alertmanager.). Install oauth2-proxy helm chart and modify the default values. helm repo add --force-update oauth2-proxy https://oauth2-proxy.github.io/manifests helm upgrade --install --version 4.2.0 --namespace oauth2-proxy-keycloak --create-namespace --values - oauth2-proxy oauth2-proxy/oauth2-proxy \u003c\u003c EOF config: clientID: oauth2-proxy-keycloak.${CLUSTER_FQDN} clientSecret: \"${MY_PASSWORD}\" cookieSecret: \"$(openssl rand -base64 32 | head -c 32 | base64)\" configFile: |- email_domains = [ \"*\" ] upstreams = [ \"file:///dev/null\" ] whitelist_domains = \".${CLUSTER_FQDN}\" cookie_domains = \".${CLUSTER_FQDN}\" provider = \"keycloak\" login_url = \"https://keycloak.${CLUSTER_FQDN}/auth/realms/myrealm/protocol/openid-connect/auth\" redeem_url = \"https://keycloak.${CLUSTER_FQDN}/auth/realms/myrealm/protocol/openid-connect/token\" profile_url = \"https://keycloak.${CLUSTER_FQDN}/auth/realms/myrealm/protocol/openid-connect/userinfo\" validate_url = \"https://keycloak.${CLUSTER_FQDN}/auth/realms/myrealm/protocol/openid-connect/userinfo\" scope = \"openid email profile\" ssl_insecure_skip_verify = \"true\" insecure_oidc_skip_issuer_verification = \"true\" ingress: enabled: true hosts: - oauth2-proxy-keycloak.${CLUSTER_FQDN} tls: - secretName: ingress-cert-${LETSENCRYPT_ENVIRONMENT} hosts: - oauth2-proxy-keycloak.${CLUSTER_FQDN} metrics: servicemonitor: enabled: true EOF ","date":"0001-01-01","objectID":"/hugo-loveit/readme/:2:0","tags":["EKS","Image","Pipeline Templates"],"title":"Image Test EKS Post","uri":"/hugo-loveit/readme/"},{"categories":["EKS","Image","Pipelines"],"content":"Dex Install dex helm chart and modify the default values. helm repo add --force-update dex https://charts.dexidp.io helm upgrade --install --version 0.6.0 --namespace dex --create-namespace --values - dex dex/dex \u003c\u003c EOF ingress: enabled: true annotations: nginx.ingress.kubernetes.io/ssl-redirect: \"false\" hosts: - host: dex.${CLUSTER_FQDN} paths: - path: / pathType: ImplementationSpecific tls: - secretName: ingress-cert-${LETSENCRYPT_ENVIRONMENT} hosts: - dex.${CLUSTER_FQDN} config: issuer: https://dex.${CLUSTER_FQDN} storage: type: kubernetes config: inCluster: true oauth2: skipApprovalScreen: true connectors: - type: github id: github name: GitHub config: clientID: ${MY_GITHUB_ORG_OAUTH_DEX_CLIENT_ID} clientSecret: ${MY_GITHUB_ORG_OAUTH_DEX_CLIENT_SECRET} redirectURI: https://dex.${CLUSTER_FQDN}/callback orgs: - name: ${MY_GITHUB_ORG_NAME} - type: oidc id: okta name: Okta config: issuer: ${OKTA_ISSUER} clientID: ${OKTA_CLIENT_ID} clientSecret: ${OKTA_CLIENT_SECRET} redirectURI: https://dex.${CLUSTER_FQDN}/callback scopes: - openid - profile - email getUserInfo: true staticClients: - id: argocd.${CLUSTER_FQDN} redirectURIs: - https://argocd.${CLUSTER_FQDN}/auth/callback name: ArgoCD secret: ${MY_PASSWORD} - id: gangway.${CLUSTER_FQDN} redirectURIs: - https://gangway.${CLUSTER_FQDN}/callback name: Gangway secret: ${MY_PASSWORD} - id: harbor.${CLUSTER_FQDN} redirectURIs: - https://harbor.${CLUSTER_FQDN}/c/oidc/callback name: Harbor secret: ${MY_PASSWORD} - id: kiali.${CLUSTER_FQDN} redirectURIs: - https://kiali.${CLUSTER_FQDN} name: Kiali secret: ${MY_PASSWORD} - id: keycloak.${CLUSTER_FQDN} redirectURIs: - https://keycloak.${CLUSTER_FQDN}/auth/realms/myrealm/broker/dex/endpoint name: Keycloak secret: ${MY_PASSWORD} - id: oauth2-proxy.${CLUSTER_FQDN} redirectURIs: - https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/callback name: OAuth2 Proxy secret: ${MY_PASSWORD} - id: vault.${CLUSTER_FQDN} redirectURIs: - https://vault.${CLUSTER_FQDN}/ui/vault/auth/oidc/oidc/callback - http://localhost:8250/oidc/callback name: Vault secret: ${MY_PASSWORD} enablePasswordDB: false EOF ","date":"0001-01-01","objectID":"/hugo-loveit/readme/:3:0","tags":["EKS","Image","Pipeline Templates"],"title":"Image Test EKS Post","uri":"/hugo-loveit/readme/"},{"categories":["EKS","Image","Pipelines"],"content":"oauth2-proxy Install oauth2-proxy to secure the endpoints like (prometheus., alertmanager.). Install oauth2-proxy helm chart and modify the default values. helm upgrade --install --version 4.2.0 --namespace oauth2-proxy --create-namespace --values - oauth2-proxy oauth2-proxy/oauth2-proxy \u003c\u003c EOF config: clientID: oauth2-proxy.${CLUSTER_FQDN} clientSecret: \"${MY_PASSWORD}\" cookieSecret: \"$(openssl rand -base64 32 | head -c 32 | base64)\" configFile: |- email_domains = [ \"*\" ] upstreams = [ \"file:///dev/null\" ] whitelist_domains = \".${CLUSTER_FQDN}\" cookie_domains = \".${CLUSTER_FQDN}\" provider = \"oidc\" oidc_issuer_url = \"https://dex.${CLUSTER_FQDN}\" ssl_insecure_skip_verify = \"true\" insecure_oidc_skip_issuer_verification = \"true\" ingress: enabled: true hosts: - oauth2-proxy.${CLUSTER_FQDN} tls: - secretName: ingress-cert-${LETSENCRYPT_ENVIRONMENT} hosts: - oauth2-proxy.${CLUSTER_FQDN} metrics: servicemonitor: enabled: true EOF ","date":"0001-01-01","objectID":"/hugo-loveit/readme/:4:0","tags":["EKS","Image","Pipeline Templates"],"title":"Image Test EKS Post","uri":"/hugo-loveit/readme/"},{"categories":["EKS","Image","Pipelines"],"content":"Gangway Install gangway: helm repo add --force-update stable https://charts.helm.sh/stable helm upgrade --install --version 0.4.5 --namespace gangway --create-namespace --values - gangway stable/gangway \u003c\u003c EOF # https://github.com/helm/charts/blob/master/stable/gangway/values.yaml trustedCACert: | $(curl -s \"${LETSENCRYPT_CERTIFICATE}\" | sed \"s/^/ /\") gangway: clusterName: ${CLUSTER_FQDN} authorizeURL: https://dex.${CLUSTER_FQDN}/auth tokenURL: https://dex.${CLUSTER_FQDN}/token audience: https://dex.${CLUSTER_FQDN}/userinfo redirectURL: https://gangway.${CLUSTER_FQDN}/callback clientID: gangway.${CLUSTER_FQDN} clientSecret: ${MY_PASSWORD} apiServerURL: https://kube-oidc-proxy.${CLUSTER_FQDN} ingress: enabled: true annotations: nginx.ingress.kubernetes.io/auth-url: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/auth nginx.ingress.kubernetes.io/auth-signin: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/start?rd=\\$scheme://\\$host\\$request_uri hosts: - gangway.${CLUSTER_FQDN} tls: - secretName: ingress-cert-${LETSENCRYPT_ENVIRONMENT} hosts: - gangway.${CLUSTER_FQDN} EOF ","date":"0001-01-01","objectID":"/hugo-loveit/readme/:5:0","tags":["EKS","Image","Pipeline Templates"],"title":"Image Test EKS Post","uri":"/hugo-loveit/readme/"},{"categories":["EKS","Image","Pipelines"],"content":"kube-oidc-proxy The kube-oidc-proxy accepting connections only via HTTPS. It’s necessary to configure ingress to communicate with the backend over HTTPS. Install kube-oidc-proxy: test -d \"tmp/${CLUSTER_FQDN}/kube-oidc-proxy\" || git clone --quiet https://github.com/jetstack/kube-oidc-proxy.git \"tmp/${CLUSTER_FQDN}/kube-oidc-proxy\" git -C \"tmp/${CLUSTER_FQDN}/kube-oidc-proxy\" checkout --quiet v0.3.0 helm upgrade --install --namespace kube-oidc-proxy --create-namespace --values - kube-oidc-proxy \"tmp/${CLUSTER_FQDN}/kube-oidc-proxy/deploy/charts/kube-oidc-proxy\" \u003c\u003c EOF # https://github.com/jetstack/kube-oidc-proxy/blob/master/deploy/charts/kube-oidc-proxy/values.yaml oidc: clientId: gangway.${CLUSTER_FQDN} issuerUrl: https://dex.${CLUSTER_FQDN} usernameClaim: email caPEM: | $(curl -s \"${LETSENCRYPT_CERTIFICATE}\" | sed \"s/^/ /\") ingress: annotations: nginx.ingress.kubernetes.io/backend-protocol: HTTPS enabled: true hosts: - host: kube-oidc-proxy.${CLUSTER_FQDN} paths: - / tls: - secretName: ingress-cert-${LETSENCRYPT_ENVIRONMENT} hosts: - kube-oidc-proxy.${CLUSTER_FQDN} EOF If you get the credentials form the https://gangway.kube1.k8s.mylabs.dev you will have the access to the cluster, but no rights there. Add access rights to the user: kubectl apply -f - \u003c\u003c EOF apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: kube-prometheus-stack name: secret-reader rules: - apiGroups: [\"\"] resources: [\"secrets\"] verbs: [\"get\", \"watch\", \"list\"] --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: read-secrets namespace: kube-prometheus-stack subjects: - kind: User name: ${MY_EMAIL} apiGroup: rbac.authorization.k8s.io roleRef: kind: Role name: secret-reader apiGroup: rbac.authorization.k8s.io --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: pods-reader rules: - apiGroups: [\"\"] resources: [\"pods\"] verbs: [\"get\", \"watch\", \"list\"] --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: read-pods subjects: - kind: User name: ${MY_EMAIL} apiGroup: rbac.authorization.k8s.io roleRef: kind: ClusterRole name: pods-reader apiGroup: rbac.authorization.k8s.io EOF The user should be able to read the secrets in kube-prometheus-stack namespace: kubectl describe secrets --insecure-skip-tls-verify -n kube-prometheus-stack \"ingress-cert-${LETSENCRYPT_ENVIRONMENT}\" # DevSkim: ignore DS126188 Output: Name: ingress-cert-staging Namespace: kube-prometheus-stack Labels: kubed.appscode.com/origin.cluster=kube1.k8s.mylabs.dev kubed.appscode.com/origin.name=ingress-cert-staging kubed.appscode.com/origin.namespace=cert-manager Annotations: cert-manager.io/alt-names: *.kube1.k8s.mylabs.dev,kube1.k8s.mylabs.dev cert-manager.io/certificate-name: ingress-cert-staging cert-manager.io/common-name: *.kube1.k8s.mylabs.dev cert-manager.io/ip-sans: cert-manager.io/issuer-group: cert-manager.io/issuer-kind: ClusterIssuer cert-manager.io/issuer-name: letsencrypt-staging-dns cert-manager.io/uri-sans: kubed.appscode.com/origin: {\"namespace\":\"cert-manager\",\"name\":\"ingress-cert-staging\",\"uid\":\"f1ed062c-23d9-4cf7-ad51-cfafd8a3b788\",\"resourceVersion\":\"5296\"} Type: kubernetes.io/tls Data ==== tls.crt: 3586 bytes tls.key: 1679 bytes But it’s not allowed to delete the secrets for the user: kubectl delete secrets --insecure-skip-tls-verify -n kube-prometheus-stack \"ingress-cert-${LETSENCRYPT_ENVIRONMENT}\" # DevSkim: ignore DS126188 Output: Error from server (Forbidden): secrets \"ingress-cert-staging\" is forbidden: User \"petr.ruzicka@gmail.com\" cannot delete resource \"secrets\" in API group \"\" in the namespace \"kube-prometheus-stack\" The user can not read secrets outside the kube-prometheus-stack: kubectl get secrets --insecure-skip-tls-verify -n kube-system # DevSkim: ignore DS126188 Output: Error from server (Forbidden): secrets is forbidden: User \"petr.ruzicka@gmail.com\" cannot list resource \"secrets\" in API group \"\" in the namespace \"kube-system\" You can see the pods “everywhere”: kub","date":"0001-01-01","objectID":"/hugo-loveit/readme/:6:0","tags":["EKS","Image","Pipeline Templates"],"title":"Image Test EKS Post","uri":"/hugo-loveit/readme/"}]